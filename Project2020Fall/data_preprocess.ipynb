{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocess.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOprdmGc4wJ49Q9aI/yLj06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangxuanWu/Python/blob/master/Project2020Fall/data_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RABRYncayBjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ast\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "from pickle import dump\n",
        "\n",
        "import numpy as np\n",
        "from tfsnippet.utils import makedirs\n",
        "\n",
        "output_folder = 'processed'\n",
        "makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_and_save(category, filename, dataset, dataset_folder):\n",
        "    temp = np.genfromtxt(os.path.join(dataset_folder, category, filename),\n",
        "                         dtype=np.float32,\n",
        "                         delimiter=',')\n",
        "    print(dataset, category, filename, temp.shape)\n",
        "    with open(os.path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
        "        dump(temp, file)\n",
        "\n",
        "\n",
        "def load_data(dataset):\n",
        "    if dataset == 'SMD':\n",
        "        dataset_folder = 'ServerMachineDataset'\n",
        "        file_list = os.listdir(os.path.join(dataset_folder, \"train\"))\n",
        "        for filename in file_list:\n",
        "            if filename.endswith('.txt'):\n",
        "                load_and_save('train', filename, filename.strip('.txt'), dataset_folder)\n",
        "                load_and_save('test', filename, filename.strip('.txt'), dataset_folder)\n",
        "                load_and_save('test_label', filename, filename.strip('.txt'), dataset_folder)\n",
        "    elif dataset == 'SMAP' or dataset == 'MSL':\n",
        "        dataset_folder = 'data'\n",
        "        with open(os.path.join(dataset_folder, 'labeled_anomalies.csv'), 'r') as file:\n",
        "            csv_reader = csv.reader(file, delimiter=',')\n",
        "            res = [row for row in csv_reader][1:]\n",
        "        res = sorted(res, key=lambda k: k[0])\n",
        "        label_folder = os.path.join(dataset_folder, 'test_label')\n",
        "        makedirs(label_folder, exist_ok=True)\n",
        "        data_info = [row for row in res if row[1] == dataset and row[0] != 'P-2']\n",
        "        labels = []\n",
        "        for row in data_info:\n",
        "            anomalies = ast.literal_eval(row[2])\n",
        "            length = int(row[-1])\n",
        "            label = np.zeros([length], dtype=np.bool)\n",
        "            for anomaly in anomalies:\n",
        "                label[anomaly[0]:anomaly[1] + 1] = True\n",
        "            labels.extend(label)\n",
        "        labels = np.asarray(labels)\n",
        "        print(dataset, 'test_label', labels.shape)\n",
        "        with open(os.path.join(output_folder, dataset + \"_\" + 'test_label' + \".pkl\"), \"wb\") as file:\n",
        "            dump(labels, file)\n",
        "\n",
        "        def concatenate_and_save(category):\n",
        "            data = []\n",
        "            for row in data_info:\n",
        "                filename = row[0]\n",
        "                temp = np.load(os.path.join(dataset_folder, category, filename + '.npy'))\n",
        "                data.extend(temp)\n",
        "            data = np.asarray(data)\n",
        "            print(dataset, category, data.shape)\n",
        "            with open(os.path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
        "                dump(data, file)\n",
        "\n",
        "        for c in ['train', 'test']:\n",
        "            concatenate_and_save(c)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    datasets = ['SMD', 'SMAP', 'MSL']\n",
        "    commands = sys.argv[1:]\n",
        "    load = []\n",
        "    if len(commands) > 0:\n",
        "        for d in commands:\n",
        "            if d in datasets:\n",
        "                load_data(d)\n",
        "    else:\n",
        "        print(\"\"\"\n",
        "        Usage: python data_preprocess.py <datasets>\n",
        "        where <datasets> should be one of ['SMD', 'SMAP', 'MSL']\n",
        "        \"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}