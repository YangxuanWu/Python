{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMup+WOu+FD4d7lqQZTaIxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangxuanWu/Python/blob/master/Project2020Fall/%E7%BB%83%E4%B9%A0%EF%BC%88project%EF%BC%89/project_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnYLQ0vrPsZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "369c6dc1-2970-4d46-d0e2-5f876c4be1bb"
      },
      "source": [
        "%pip install git+https://github.com/thu-ml/zhusuan.git\n",
        "%pip install git+https://github.com/haowen-xu/tfsnippet.git@v0.2.0-alpha1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/thu-ml/zhusuan.git\n",
            "  Cloning https://github.com/thu-ml/zhusuan.git to /tmp/pip-req-build-vcmn51ju\n",
            "  Running command git clone -q https://github.com/thu-ml/zhusuan.git /tmp/pip-req-build-vcmn51ju\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from zhusuan==0.4.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from zhusuan==0.4.0) (1.15.0)\n",
            "Building wheels for collected packages: zhusuan\n",
            "  Building wheel for zhusuan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zhusuan: filename=zhusuan-0.4.0-py2.py3-none-any.whl size=73591 sha256=08d32d74bbf8e14237eef49f76dd290241c28b461a4ec0f4a50187fe4cf0e62d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dutu9u4z/wheels/45/cb/f5/bfd913ae94924c3151ac7d20dab61be39e90a2b07bdc6cb75e\n",
            "Successfully built zhusuan\n",
            "Installing collected packages: zhusuan\n",
            "Successfully installed zhusuan-0.4.0\n",
            "Collecting git+https://github.com/haowen-xu/tfsnippet.git@v0.2.0-alpha1\n",
            "  Cloning https://github.com/haowen-xu/tfsnippet.git (to revision v0.2.0-alpha1) to /tmp/pip-req-build-55ax8ce9\n",
            "  Running command git clone -q https://github.com/haowen-xu/tfsnippet.git /tmp/pip-req-build-55ax8ce9\n",
            "  Running command git checkout -q 7b43abdbdd29f1914dbc11b961b5d45b9de76653\n",
            "Requirement already satisfied: filelock>=3.0.10 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (3.0.12)\n",
            "Collecting frozendict>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/55/a12ded2c426a4d2bee73f88304c9c08ebbdbadb82569ebdd6a0c007cfd08/frozendict-1.2.tar.gz\n",
            "Collecting idx2numpy>=1.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/23/6b/abab4652eb249f432c62431907c8de32bdcedb5abdf869ff86653efff981/idx2numpy-1.2.2.tar.gz\n",
            "Collecting lazy-object-proxy>=1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7c/f6d5cb5f3d60d340958c8a70fc9ce356c900c7da3efc75ab6a870a90cde9/lazy_object_proxy-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: natsort>=5.3.3 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (1.18.5)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (3.13)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (1.4.1)\n",
            "Collecting semver>=2.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/94/19/275d10576ad1b19b801efa478182d7352d244a40164162334010e3a948d5/semver-2.10.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.2.0a1) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.2.0a1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.2.0a1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.2.0a1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.2.0a1) (2.10)\n",
            "Building wheels for collected packages: TFSnippet, frozendict, idx2numpy\n",
            "  Building wheel for TFSnippet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TFSnippet: filename=TFSnippet-0.2.0a1-cp36-none-any.whl size=207813 sha256=8e416ca1bc35e3687e9b7155b4d86275b12831a0c3b7543e05169dcae8be0299\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0cd_deyb/wheels/f0/23/4b/05b970ddc6d0215ba1938068d3e505283d7f7fb0e2d0792252\n",
            "  Building wheel for frozendict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for frozendict: filename=frozendict-1.2-cp36-none-any.whl size=3149 sha256=19ba4922ee3728e38e911b53fc5c0c28e8e6f951721cfa49a1b1964a038579f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/6c/e9/534386165bd12cf1885582c75eb6d0ffcb321b65c23fe0f834\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.2-cp36-none-any.whl size=8032 sha256=6515bc6189e679476bf4304a1817f57ff6be2f6b05dd7e86d62c1696d391ab9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/b5/69/3e0757b3086607e95db70661798fdf98a77a0bb79c54e1f320\n",
            "Successfully built TFSnippet frozendict idx2numpy\n",
            "Installing collected packages: frozendict, idx2numpy, lazy-object-proxy, semver, TFSnippet\n",
            "Successfully installed TFSnippet-0.2.0a1 frozendict-1.2 idx2numpy-1.2.2 lazy-object-proxy-1.5.1 semver-2.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_CnpQ6SQfym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cd338f9f-c7b0-4555-ec62-2ce0ecf30cc3"
      },
      "source": [
        "%pip install matplotlib==3.1.1\n",
        "%pip install pandas==1.0.0\n",
        "%pip install numpy==1.16.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib==3.1.1 in /usr/local/lib/python3.6/dist-packages (3.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.1.1) (1.15.0)\n",
            "Requirement already satisfied: pandas==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.0) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy==1.16.0 in /usr/local/lib/python3.6/dist-packages (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7CctkDTTlLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "042c849e-c850-4176-e446-4353cc3801cc"
      },
      "source": [
        "%pip install scipy==1.2.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ildld5YGTmUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "38090da3-10d7-452a-fc7a-33deb9d3c8dc"
      },
      "source": [
        "%pip install scikit_learn==0.20.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit_learn==0.20.2 in /usr/local/lib/python3.6/dist-packages (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit_learn==0.20.2) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit_learn==0.20.2) (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw0cCIznTnm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "dd7c077c-b750-4e89-aafc-2b1611a647f3"
      },
      "source": [
        "%pip install tensorflow-gpu==1.12.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.12.0 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.12.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (1.16.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.0) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.12.0) (49.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.12.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt25I_f3Toki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6cf188cf-34af-45c3-c97c-317ec8459e5e"
      },
      "source": [
        "%pip install tensorflow_probability==0.5.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.5.0) (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbP2u4rBTopZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "20186f08-770e-45b1-aff0-c5074a1b2de6"
      },
      "source": [
        "%pip install tqdm==4.48.2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tqdm==4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 1.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.48.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X8IyWSuTory",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9a0a8ba6-3fc8-44ac-d0bd-c6560362f8e3"
      },
      "source": [
        "%pip install imageio==2.4.1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.1) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.1) (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqNDbUUGTouQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "634d19d9-a0ee-480f-fca8-b02317c49158"
      },
      "source": [
        "%pip install fs==2.3.0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fs==2.3.0 in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from fs==2.3.0) (49.2.0)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.6/dist-packages (from fs==2.3.0) (1.4.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fs==2.3.0) (2018.9)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from fs==2.3.0) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg4qUtYpTr-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57c1812c-310b-4e37-a543-faae8aa4c013"
      },
      "source": [
        "%pip install click==7.0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: click==7.0 in /usr/local/lib/python3.6/dist-packages (7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abgZFJN8U9k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74aa7780-74e2-424a-e557-34ae9c0d2ee3"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "from tfsnippet import Distribution, Normal\n",
        "\n",
        "class RecurrentDistribution(Distribution):\n",
        "    \"\"\"\n",
        "    A multi-variable distribution integrated with recurrent structure.\n",
        "    \"\"\"\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self._dtype\n",
        "\n",
        "    @property\n",
        "    def is_continuous(self):\n",
        "        return self._is_continuous\n",
        "\n",
        "    @property\n",
        "    def is_reparameterized(self):\n",
        "        return self._is_reparameterized\n",
        "\n",
        "    @property\n",
        "    def value_shape(self):\n",
        "        return self.normal.value_shape\n",
        "\n",
        "    def get_value_shape(self):\n",
        "        return self.normal.get_value_shape()\n",
        "\n",
        "    @property\n",
        "    def batch_shape(self):\n",
        "        return self.normal.batch_shape\n",
        "\n",
        "    def get_batch_shape(self):\n",
        "        return self.normal.get_batch_shape()\n",
        "\n",
        "    def sample_step(self, a, t):\n",
        "        z_previous, mu_q_previous, std_q_previous = a\n",
        "        noise_n, input_q_n = t\n",
        "        input_q_n = tf.broadcast_to(input_q_n,\n",
        "                                    [tf.shape(z_previous)[0], tf.shape(input_q_n)[0], input_q_n.shape[1]])\n",
        "        input_q = tf.concat([input_q_n, z_previous], axis=-1)\n",
        "        mu_q = self.mean_q_mlp(input_q, reuse=tf.AUTO_REUSE)  # n_sample * batch_size * z_dim\n",
        "\n",
        "        std_q = self.std_q_mlp(input_q)  # n_sample * batch_size * z_dim\n",
        "\n",
        "        temp = tf.einsum('ik,ijk->ijk', noise_n, std_q)  # n_sample * batch_size * z_dim\n",
        "        mu_q = tf.broadcast_to(mu_q, tf.shape(temp))\n",
        "        std_q = tf.broadcast_to(std_q, tf.shape(temp))\n",
        "        z_n = temp + mu_q\n",
        "\n",
        "        return z_n, mu_q, std_q\n",
        "\n",
        "    # @global_reuse\n",
        "    def log_prob_step(self, _, t):\n",
        "\n",
        "        given_n, input_q_n = t\n",
        "        if len(given_n.shape) > 2:\n",
        "            input_q_n = tf.broadcast_to(input_q_n,\n",
        "                                        [tf.shape(given_n)[0], tf.shape(input_q_n)[0], input_q_n.shape[1]])\n",
        "        input_q = tf.concat([given_n, input_q_n], axis=-1)\n",
        "        mu_q = self.mean_q_mlp(input_q, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        std_q = self.std_q_mlp(input_q)\n",
        "        logstd_q = tf.log(std_q)\n",
        "        precision = tf.exp(-2 * logstd_q)\n",
        "        if self._check_numerics:\n",
        "            precision = tf.check_numerics(precision, \"precision\")\n",
        "        log_prob_n = - 0.9189385332046727 - logstd_q - 0.5 * precision * tf.square(tf.minimum(tf.abs(given_n - mu_q),\n",
        "                                                                                              1e8))\n",
        "        return log_prob_n\n",
        "\n",
        "    def __init__(self, input_q, mean_q_mlp, std_q_mlp, z_dim, window_length=100, is_reparameterized=True,\n",
        "                 check_numerics=True):\n",
        "        super(RecurrentDistribution, self).__init__()\n",
        "        self.normal = Normal(mean=tf.zeros([window_length, z_dim]), std=tf.ones([window_length, z_dim]))\n",
        "        self.std_q_mlp = std_q_mlp\n",
        "        self.mean_q_mlp = mean_q_mlp\n",
        "        self._check_numerics = check_numerics\n",
        "        self.input_q = tf.transpose(input_q, [1, 0, 2])\n",
        "        self._dtype = input_q.dtype\n",
        "        self._is_reparameterized = is_reparameterized\n",
        "        self._is_continuous = True\n",
        "        self.z_dim = z_dim\n",
        "        self.window_length = window_length\n",
        "        self.time_first_shape = tf.convert_to_tensor([self.window_length, tf.shape(input_q)[0], self.z_dim])\n",
        "\n",
        "    def sample(self, n_samples=1024, is_reparameterized=None, group_ndims=0, compute_density=False,\n",
        "               name=None):\n",
        "\n",
        "        from tfsnippet.stochastic import StochasticTensor\n",
        "        if n_samples is None:\n",
        "            n_samples = 1\n",
        "            n_samples_is_none = True\n",
        "        else:\n",
        "            n_samples_is_none = False\n",
        "        with tf.name_scope(name=name, default_name='sample'):\n",
        "            noise = self.normal.sample(n_samples=n_samples)\n",
        "\n",
        "            noise = tf.transpose(noise, [1, 0, 2])  # window_length * n_samples * z_dim\n",
        "            noise = tf.truncated_normal(tf.shape(noise))\n",
        "\n",
        "            time_indices_shape = tf.convert_to_tensor([n_samples, tf.shape(self.input_q)[1], self.z_dim])\n",
        "\n",
        "            samples = tf.scan(fn=self.sample_step,\n",
        "                              elems=(noise, self.input_q),\n",
        "                              initializer=(tf.zeros(time_indices_shape),\n",
        "                                           tf.zeros(time_indices_shape),\n",
        "                                           tf.ones(time_indices_shape)),\n",
        "                              back_prop=False\n",
        "                              )[0]  # time_step * n_samples * batch_size * z_dim\n",
        "\n",
        "            samples = tf.transpose(samples, [1, 2, 0, 3])  # n_samples * batch_size * time_step *  z_dim\n",
        "\n",
        "            if n_samples_is_none:\n",
        "                t = StochasticTensor(\n",
        "                    distribution=self,\n",
        "                    tensor=tf.reduce_mean(samples, axis=0),\n",
        "                    n_samples=1,\n",
        "                    group_ndims=group_ndims,\n",
        "                    is_reparameterized=self.is_reparameterized\n",
        "                )\n",
        "            else:\n",
        "                t = StochasticTensor(\n",
        "                    distribution=self,\n",
        "                    tensor=samples,\n",
        "                    n_samples=n_samples,\n",
        "                    group_ndims=group_ndims,\n",
        "                    is_reparameterized=self.is_reparameterized\n",
        "                )\n",
        "            if compute_density:\n",
        "                with tf.name_scope('compute_prob_and_log_prob'):\n",
        "                    log_p = t.log_prob()\n",
        "                    t._self_prob = tf.exp(log_p)\n",
        "            return t\n",
        "\n",
        "    def log_prob(self, given, group_ndims=0, name=None):\n",
        "        with tf.name_scope(name=name, default_name='log_prob'):\n",
        "            if len(given.shape) > 3:\n",
        "                time_indices_shape = tf.convert_to_tensor([tf.shape(given)[0], tf.shape(self.input_q)[1], self.z_dim])\n",
        "                given = tf.transpose(given, [2, 0, 1, 3])\n",
        "            else:\n",
        "                time_indices_shape = tf.convert_to_tensor([tf.shape(self.input_q)[1], self.z_dim])\n",
        "                given = tf.transpose(given, [1, 0, 2])\n",
        "            log_prob = tf.scan(fn=self.log_prob_step, elems=(given, self.input_q), initializer=tf.zeros(time_indices_shape), back_prop=False)\n",
        "            if len(given.shape) > 3:\n",
        "                log_prob = tf.transpose(log_prob, [1, 2, 0, 3])\n",
        "            else:\n",
        "                log_prob = tf.transpose(log_prob, [1, 0, 2])\n",
        "\n",
        "            if group_ndims == 1:\n",
        "                log_prob = tf.reduce_sum(log_prob, axis=-1)\n",
        "            return log_prob\n",
        "\n",
        "    def prob(self, given, group_ndims=0, name=None):\n",
        "        with tf.name_scope(name=name, default_name='prob'):\n",
        "            log_prob = self.log_prob(given, group_ndims, name)\n",
        "            return tf.exp(log_prob)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a47c49075313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfsnippet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCz95XXoU9tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "from tfsnippet.bayes import BayesianNet\n",
        "from tfsnippet.distributions import Distribution\n",
        "from tfsnippet.stochastic import StochasticTensor, validate_n_samples_arg\n",
        "from tfsnippet.utils import (instance_reuse, is_tensor_object, reopen_variable_scope, VarScopeObject)\n",
        "from tfsnippet.variational import VariationalChain\n",
        "\n",
        "\n",
        "class VAE(VarScopeObject):\n",
        "    \"\"\"\n",
        "    A general implementation of variational auto-encoder as module.\n",
        "\n",
        "    The variational auto-encoder (\"Auto-Encoding Variational Bayes\",\n",
        "    Kingma, D.P. and Welling) is a deep Bayesian network, with observed\n",
        "    variable `x` and latent variable `z`.  The generative process\n",
        "    starts from `z` with prior distribution :math:`p(z)`, following a\n",
        "    hidden network :math:`h(z)`, then comes to `x` with distribution\n",
        "    :math:`p(x|h(z))`.  To do posterior inference of :math:`p(z|x)`,\n",
        "    variational inference techniques are adopted, to train a separated\n",
        "    distribution :math:`q(z|h(x))` (:math:`h(x)` denoting the hidden network)\n",
        "    to approximate :math:`p(z|x)`.\n",
        "\n",
        "    This class provides a general implementation of variational auto-encoder,\n",
        "    with customizable :math:`p(z)`, :math:`p(x|h(z))`, :math:`q(z|h(x))`,\n",
        "    as well as the hidden networks :math:`h(z)` and :math:`h(x)`.\n",
        "\n",
        "    For example, to construct a VAE with diagonal Normal `z` and `x`:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        from tensorflow import keras as K\n",
        "        from tfsnippet.modules import VAE, DictMapper, Sequential\n",
        "        from tfsnippet.distributions import Normal\n",
        "\n",
        "        batch_size = 128\n",
        "        x_dims, z_dims = 100, 10\n",
        "        vae = VAE(\n",
        "            p_z=Normal(mean=tf.zeros([batch_size, z_dims]),\n",
        "                       std=tf.ones([batch_size, x_dims])),\n",
        "            p_x_given_z=Normal,\n",
        "            q_z_given_x=Normal,\n",
        "            h_for_p_x=Sequential([\n",
        "                K.layers.Dense(100, activation=tf.nn.relu),\n",
        "                DictMapper({'mean': K.layers.Dense(x_dims),\n",
        "                            'logstd': K.layers.Dense(x_dims)})\n",
        "            ]),\n",
        "            h_for_q_z=Sequential([\n",
        "                K.layers.Dense(100, activation=tf.nn.relu),\n",
        "                DictMapper({'mean': K.layers.Dense(z_dims),\n",
        "                            'logstd': K.layers.Dense(z_dims)})\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    To train the `vae`:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        # Automatically derive a single-sample loss.\n",
        "        # Depending on ``z.is_reparameterized``, it might be derived by\n",
        "        # `sgvb` (is_reparameterized == True) or `reinforce` (otherwise).\n",
        "        loss = vae.get_training_loss(x)\n",
        "\n",
        "        # Automatically derive a multi-sample loss.\n",
        "        # Depending on ``z.is_reparameterized``, it might be derived by\n",
        "        # `iwae` (is_reparameterized == True) or `vimco` (otherwise).\n",
        "        loss = vae.get_training_loss(x, n_z=10)\n",
        "\n",
        "        # Or manually derive a reweighted wake-sleep training loss.\n",
        "        # Note the `VariationalTrainingObjectives` produce per-data\n",
        "        # training objectives, instead of a 0-d scalar loss as the\n",
        "        # `VAE.get_training_loss` does.\n",
        "        chain = vae.chain(x, n_z=10)\n",
        "        loss = tf.reduce_mean(chain.vi.training.rws_wake())\n",
        "\n",
        "    To map from `x` to `z`:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        # use the :class:`Module` interface for one-to-one mapping\n",
        "        z = vae(x)\n",
        "\n",
        "        # use the :class:`Module` interface for multiple `z` samples\n",
        "        z = vae(x, n_z=10)\n",
        "\n",
        "        # or obtain the variational :class:`BayesianNet` with observed `z`\n",
        "        q_net = vae.variational(x, z=observed_z)\n",
        "        z_log_prob = q_net['z'].log_prob()\n",
        "\n",
        "    To reconstruct `x`:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        # use the :meth:`VAE.reconstruct` for obtaining one `x` sample\n",
        "        x_reconstructed = vae.reconstruct(x)\n",
        "\n",
        "        # to obtain multiple `z` samples, and further multiple `x` samples\n",
        "        # (this results in 100 `x` samples for each input `x`)\n",
        "        x_reconstructed = vae.reconstruct(x, n_z=10, n_x=10)\n",
        "\n",
        "    To sample `x` from prior `z` or observed `z`:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        # sample multiple prior `z`, then one `x` for each `z`\n",
        "        x = vae.model(n_z=10)['x']\n",
        "\n",
        "        # sample multiple `x` for each observed `z`\n",
        "        x = vae.model(z=observed_z, n_x=10)['x']\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_z, p_x_given_z, q_z_given_x, h_for_p_x, h_for_q_z,\n",
        "                 z_group_ndims=1, x_group_ndims=1, is_reparameterized=None,\n",
        "                 name=None, scope=None):\n",
        "        \"\"\"\n",
        "        Construct the :class:`VAE`.\n",
        "\n",
        "        Args:\n",
        "            p_z (Distribution): :math:`p(z)`, the distribution instance.\n",
        "            p_x_given_z: :math:`p(x|h(z))`, a distribution class or\n",
        "                a :class:`DistributionFactory` object.\n",
        "            q_z_given_x: :math:`q(z|h(x))`, a distribution class or\n",
        "                a :class:`DistributionFactory` object.\n",
        "            h_for_p_x (Module): :math:`h(z)`, the hidden network module for\n",
        "                :math:`p(x|h(z))`. The output of `h_for_p_x` must be a\n",
        "                ``dict[str, any]``, the parameters for `p_x_given_z`.\n",
        "            h_for_q_z (Module): :math:`h(x)`, the hidden network module for\n",
        "                :math:`q(z|h(x))`. The output of `h_for_q_z` must be a\n",
        "                ``dict[str, any]``, the parameters for `q_z_given_x`.\n",
        "            z_group_ndims (int or tf.Tensor): `group_ndims` for `z`. (default 1)\n",
        "            x_group_ndims (int or tf.Tensor): `group_ndims` for `x`. (default 1)\n",
        "            is_reparameterized (bool or None): Whether or not `z` should be\n",
        "                re-parameterized? (default :obj:`None`, following the settings\n",
        "                of z distributions.)\n",
        "            name (str): Optional name of this module\n",
        "                (argument of :class:`~tfsnippet.utils.VarScopeObject`).\n",
        "            scope (str): Optional scope of this module\n",
        "                (argument of :class:`~tfsnippet.utils.VarScopeObject`).\n",
        "\n",
        "        See Also:\n",
        "            :meth:`tfsnippet.distributions.Distribution.log_prob` for\n",
        "                contents about `group_ndims`.\n",
        "        \"\"\"\n",
        "        if not isinstance(p_z, Distribution):\n",
        "            raise TypeError('`p_z` must be an instance of `Distribution`')\n",
        "        if not callable(h_for_p_x):\n",
        "            raise TypeError('`h_for_p_x` must be an instance of `Module` or '\n",
        "                            'a callable object')\n",
        "        if not callable(h_for_q_z):\n",
        "            raise TypeError('`h_for_q_z` must be an instance of `Module` or '\n",
        "                            'a callable object')\n",
        "        super(VAE, self).__init__(name=name, scope=scope)\n",
        "\n",
        "        # Defensive coding: wrap `h_for_p_x` and `h_for_q_z` in reused scope.\n",
        "        if not isinstance(h_for_p_x, VarScopeObject):\n",
        "            with reopen_variable_scope(self.variable_scope):\n",
        "                h_for_p_x = Lambda(h_for_p_x, name='h_for_p_x')\n",
        "        if not isinstance(h_for_q_z, VarScopeObject):\n",
        "            with reopen_variable_scope(self.variable_scope):\n",
        "                h_for_q_z = Lambda(h_for_q_z, name='h_for_q_z')\n",
        "\n",
        "        self._p_z = p_z\n",
        "        self._p_x_given_z = p_x_given_z\n",
        "        self._q_z_given_x = q_z_given_x\n",
        "        self._h_for_p_x = h_for_p_x\n",
        "        self._h_for_q_z = h_for_q_z\n",
        "        self._z_group_ndims = z_group_ndims\n",
        "        self._x_group_ndims = x_group_ndims\n",
        "        self._is_reparameterized = is_reparameterized\n",
        "\n",
        "    def __call__(self, inputs, **kwargs):\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            # Here `reopen_name_scope` is set to True, so that multiple\n",
        "            # calls to the same Module instance will always generate operations\n",
        "            # within the original name scope.\n",
        "            # However, in order for ``tf.variable_scope(default_name=...)``\n",
        "            # to work properly with variable reusing, we must generate a nested\n",
        "            # unique name scope.\n",
        "            with tf.name_scope('forward'):\n",
        "                return self._forward(inputs, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def p_z(self):\n",
        "        \"\"\"\n",
        "        Get :math:`p(z)`, the prior distribution of `z`.\n",
        "\n",
        "        Returns:\n",
        "            Distribution: The distribution instance.\n",
        "        \"\"\"\n",
        "        return self._p_z\n",
        "\n",
        "    @property\n",
        "    def p_x_given_z(self):\n",
        "        \"\"\"\n",
        "        Get the factory for :math:`p(x|h(z))`.\n",
        "\n",
        "        Returns:\n",
        "            DistributionFactory: The distribution factory.\n",
        "        \"\"\"\n",
        "        return self._p_x_given_z\n",
        "\n",
        "    @property\n",
        "    def q_z_given_x(self):\n",
        "        \"\"\"\n",
        "        Get the factory for :math:`q(z|h(x))`.\n",
        "\n",
        "        Returns:\n",
        "            DistributionFactory: The distribution factory.\n",
        "        \"\"\"\n",
        "        return self._q_z_given_x\n",
        "\n",
        "    @property\n",
        "    def h_for_p_x(self):\n",
        "        \"\"\"\n",
        "        Get :math:`h(z)`, the hidden network for :math:`p(x|h(z))`.\n",
        "\n",
        "        Returns:\n",
        "            Module: The hidden network.\n",
        "        \"\"\"\n",
        "        return self._h_for_p_x\n",
        "\n",
        "    @property\n",
        "    def h_for_q_z(self):\n",
        "        \"\"\"\n",
        "        Get :math:`h(x)`, the hidden network for :math:`q(z|h(x))`.\n",
        "\n",
        "        Returns:\n",
        "            Module: The hidden network.\n",
        "        \"\"\"\n",
        "        return self._h_for_q_z\n",
        "\n",
        "    @property\n",
        "    def z_group_ndims(self):\n",
        "        \"\"\"Get the `group_ndims` for `z`.\"\"\"\n",
        "        return self._z_group_ndims\n",
        "\n",
        "    @property\n",
        "    def x_group_ndims(self):\n",
        "        \"\"\"Get the `group_ndims` for `x`.\"\"\"\n",
        "        return self._x_group_ndims\n",
        "\n",
        "    @property\n",
        "    def is_reparameterized(self):\n",
        "        \"\"\"Whether or not `z` is re-parameterized?\"\"\"\n",
        "        return self._is_reparameterized\n",
        "\n",
        "    @instance_reuse\n",
        "    def variational(self, x, z=None, n_z=None, posterior_flow=None):\n",
        "        \"\"\"\n",
        "        Derive an instance of :math:`q(z|h(x))`, the variational net.\n",
        "\n",
        "        Args:\n",
        "            x: The observation `x` for the variational net.\n",
        "            z: If specified, observe `z` in the variational net.\n",
        "                (default :obj:`None`)\n",
        "            n_z: The number of `z` samples to take for each `x`, if `z`\n",
        "                is not observed. (default :obj:`None`, one sample for\n",
        "                each `x`, without dedicated sampling dimension)\n",
        "\n",
        "                It is recommended to specify this argument even if `z`\n",
        "                is observed, to make explicit how many samples are there\n",
        "                in the observation.\n",
        "\n",
        "        Returns:\n",
        "            BayesianNet: The variational net.\n",
        "        \"\"\"\n",
        "        observed = {}\n",
        "        if z is not None:\n",
        "            observed['z'] = z\n",
        "        net = BayesianNet(observed=observed)\n",
        "        with tf.variable_scope('h_for_q_z'):\n",
        "            z_params = self.h_for_q_z(x)\n",
        "        with tf.variable_scope('q_z_given_x'):\n",
        "            q_z_given_x = self.q_z_given_x(**z_params)\n",
        "            assert (isinstance(q_z_given_x, Distribution))\n",
        "        with tf.name_scope('z'):\n",
        "            z = net.add('z', q_z_given_x, n_samples=n_z,\n",
        "                        group_ndims=self.z_group_ndims,\n",
        "                        is_reparameterized=self.is_reparameterized,\n",
        "                        flow=posterior_flow)\n",
        "        return net\n",
        "\n",
        "    @instance_reuse\n",
        "    def model(self, z=None, x=None, n_z=None, n_x=None):\n",
        "        \"\"\"\n",
        "        Derive an instance of :math:`p(x|h(z))`, the model net.\n",
        "\n",
        "        Args:\n",
        "            z: If specified, observe `z` in the model net. (default :obj:`None`)\n",
        "            x: If specified, observe `x` in the model net. (default :obj:`None`)\n",
        "            n_z: The number of `z` samples to take for each `x`, if `z`\n",
        "                is not observed. (default :obj:`None`, one `z` sample for\n",
        "                each `x`, without dedicated sampling dimension)\n",
        "\n",
        "                It is recommended to specify this argument even if `z`\n",
        "                is observed, to make explicit how many samples are there\n",
        "                in the observation.\n",
        "            n_x: The number of `x` samples to take for each `z`, if `x`\n",
        "                is not observed. (default :obj:`None`, one `x` sample for\n",
        "                each `z`, without dedicated sampling dimension)\n",
        "\n",
        "                It is recommended to specify this argument even if `x`\n",
        "                is observed, to make explicit how many samples are there\n",
        "                in the observation.\n",
        "\n",
        "        Returns:\n",
        "            BayesianNet: The variational net.\n",
        "        \"\"\"\n",
        "        observed = {k: v for k, v in [('z', z), ('x', x)] if v is not None}\n",
        "        net = BayesianNet(observed=observed)\n",
        "        with tf.name_scope('z'):\n",
        "            z = net.add('z', self.p_z, n_samples=n_z,\n",
        "                        group_ndims=self.z_group_ndims,\n",
        "                        is_reparameterized=self.is_reparameterized)\n",
        "        with tf.variable_scope('h_for_p_x'):\n",
        "            x_params = self.h_for_p_x(z)\n",
        "        with tf.variable_scope('p_x_given_z'):\n",
        "            p_x_given_z = self.p_x_given_z(**x_params)\n",
        "            assert (isinstance(p_x_given_z, Distribution))\n",
        "        with tf.name_scope('x'):\n",
        "            x = net.add('x', p_x_given_z, n_samples=n_x,\n",
        "                        group_ndims=self.x_group_ndims)\n",
        "        return net\n",
        "\n",
        "    def chain(self, x, n_z=None, posterior_flow=None):\n",
        "        \"\"\"\n",
        "        Chain :math:`q(z|h(x))` and :math:`p(x,z|h(x))` together.\n",
        "\n",
        "        This method chains the variational net :math:`q(z|h(x))` and the\n",
        "        model net :math:`p(x,z|h(x))` together, with specified observation\n",
        "        `x`.  It is typically used to derive the training objectives of VAE.\n",
        "        It can also be used to calculate the `reconstruction probability`\n",
        "        (\"Variational Autoencoder based Anomaly Detection using Reconstruction\n",
        "        Probability\", An, J. and Cho, S. 2015) of `x`.\n",
        "\n",
        "        Notes:\n",
        "            The constructed :class:`~tfsnippet.variational.VariationalChain`\n",
        "            have `x` observed in its `model` net, thus this method cannot\n",
        "            be used to get reconstructed samples.  Use :meth:`reconstruct`\n",
        "            instead to obtain `x` samples.\n",
        "\n",
        "        Args:\n",
        "            x: The input observation `x`.\n",
        "            n_z: Number of `z` samples to take. (default :obj:`None`)\n",
        "\n",
        "        Returns:\n",
        "            VariationalChain: The variational chain.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('VAE.chain'):\n",
        "            q_net = self.variational(x, n_z=n_z, posterior_flow=posterior_flow)\n",
        "\n",
        "            # automatically detect the `latent_axis` for this chain\n",
        "            if n_z is not None:\n",
        "                latent_axis = 0\n",
        "            else:\n",
        "                latent_axis = None\n",
        "\n",
        "            chain = q_net.variational_chain(\n",
        "                lambda observed: self.model(n_z=n_z, n_x=None, **observed),\n",
        "                latent_axis=latent_axis,\n",
        "                observed={'x': x}\n",
        "            )\n",
        "        return chain\n",
        "\n",
        "    def get_training_loss(self, x, n_z=None):\n",
        "        \"\"\"\n",
        "        Get the training loss for this VAE.\n",
        "\n",
        "        The variational solver is automatically chosen according to\n",
        "        `z.is_reparameterized`, and the argument `n_z`, by the following rules:\n",
        "\n",
        "        1. If `z.is_reparameterized` is :obj:`True`, then:\n",
        "\n",
        "            1. If `n_z` > 1, use `iwae`.\n",
        "            2. If `n_z` == 1 or `n_z` is :obj:`None`, use `sgvb`.\n",
        "\n",
        "        2. If `z.is_reparameterized` is :obj:`False`, then:\n",
        "\n",
        "            1. If `n_z` > 1, use `vimco`.\n",
        "            2. If `n_z` == 1 or `n_z` is :obj:`None`, use `reinforce`.\n",
        "\n",
        "        Dynamic `n_z` is not supported by this method.  Also, Reweighted\n",
        "        Wake-Sleep algorithm is not a choice of this method.  To derive\n",
        "        the training loss for either situation, use :meth:`chain`\n",
        "        to obtain a :class:`~tfsnippet.variational.VariationalChain`,\n",
        "        and further obtain the loss by `chain.vi.training.[algorithm]`.\n",
        "\n",
        "        Args:\n",
        "            x: The input observation `x`.\n",
        "            n_z (int or None): Number of `z` samples to take.  Must be\n",
        "                :obj:`None` or a constant integer.  Dynamic tensors are not\n",
        "                accepted, since we cannot automatically choose a variational\n",
        "                solver for undeterministic `n_z`. (default :obj:`None`)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A 0-d tensor, the training loss which can be optimized\n",
        "                by gradient descent.\n",
        "\n",
        "        See Also:\n",
        "            :class:`tfsnippet.variational.VariationalChain`,\n",
        "            :class:`tfsnippet.variational.VariationalTrainingObjectives`\n",
        "        \"\"\"\n",
        "        with tf.name_scope('VAE.get_training_loss'):\n",
        "            if n_z is not None:\n",
        "                if is_tensor_object(n_z):\n",
        "                    raise TypeError('Cannot choose the variational solver '\n",
        "                                    'automatically for dynamic `n_z`')\n",
        "                n_z = validate_n_samples_arg(n_z, 'n_z')\n",
        "\n",
        "            # derive the variational chain\n",
        "            chain = self.chain(x, n_z)\n",
        "            z = chain.variational['z']\n",
        "\n",
        "            # auto choose a variational solver for training loss\n",
        "            if n_z is not None and n_z > 1:\n",
        "                if z.is_reparameterized:\n",
        "                    solver = chain.vi.training.iwae\n",
        "                else:\n",
        "                    solver = chain.vi.training.vimco\n",
        "            else:\n",
        "                if z.is_reparameterized:\n",
        "                    solver = chain.vi.training.sgvb\n",
        "                else:\n",
        "                    solver = chain.vi.training.reinforce\n",
        "\n",
        "            # derive the training loss\n",
        "            return tf.reduce_mean(solver())\n",
        "\n",
        "    def reconstruct(self, x, n_z=None, n_x=None, posterior_flow=None):\n",
        "        \"\"\"\n",
        "        Sample reconstructed `x` from :math:`p(x|h(z))`, where `z` is (are)\n",
        "        sampled from :math:`q(z|h(x))` using the specified observation `x`.\n",
        "\n",
        "        Args:\n",
        "            x: The observation `x` for :math:`q(z|h(x))`.\n",
        "            n_z: Number of intermediate `z` samples to take for each input `x`.\n",
        "            n_x: Number of reconstructed `x` samples to take for each `z`.\n",
        "\n",
        "        Returns:\n",
        "            StochasticTensor: The reconstructed samples `x`.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('VAE.reconstruct'):\n",
        "            q_net = self.variational(x, n_z=n_z, posterior_flow=posterior_flow)\n",
        "            model = self.model(z=q_net['z'], n_z=n_z, n_x=n_x)\n",
        "            return model['x']\n",
        "\n",
        "    def _forward(self, inputs, n_z=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Get a `z` sample from :math:`q(z|h(x))`, using the variational net.\n",
        "\n",
        "        Args:\n",
        "            inputs: The input `x`.\n",
        "            n_z: Number of samples to taken for `z`. (default :obj:`None`)\n",
        "            \\**kwargs: Capturing and ignoring all other parameters.  This is\n",
        "                the default behavior of a :class:`Module`.\n",
        "\n",
        "        Returns:\n",
        "            StochasticTensor: The `z` samples.\n",
        "        \"\"\"\n",
        "        q_net = self.variational(inputs, z=None, n_z=n_z, **kwargs)\n",
        "        return q_net['z']\n",
        "\n",
        "\n",
        "class Lambda(VarScopeObject):\n",
        "    \"\"\"\n",
        "    Wrapping arbitrary function into a neural network :class:`Module`.\n",
        "\n",
        "    This class wraps an arbitrary function or lambda expression into\n",
        "    a neural network :class:`Module`, reusing the variables created\n",
        "    within the specified function.\n",
        "\n",
        "    For example, one may wrap :func:`tensorflow.contrib.layers.fully_connected`\n",
        "    into a reusable module with :class:`Lambda` component as follows:\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        import functools\n",
        "        from tensorflow.contrib import layers\n",
        "\n",
        "        dense = Lambda(\n",
        "            functools.partial(\n",
        "                layers.fully_connected,\n",
        "                num_outputs=100,\n",
        "                activation_fn=tf.nn.relu\n",
        "            )\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, f, name=None, scope=None):\n",
        "        \"\"\"\n",
        "        Construct the :class:`Lambda`.\n",
        "\n",
        "        Args:\n",
        "            f ((inputs, \\**kwargs) -> outputs): The function or lambda\n",
        "                expression which derives the outputs.\n",
        "            name (str): Optional name of this module\n",
        "                (argument of :class:`~tfsnippet.utils.VarScopeObject`).\n",
        "            scope (str): Optional scope of this module\n",
        "                (argument of :class:`~tfsnippet.utils.VarScopeObject`).\n",
        "        \"\"\"\n",
        "        super(Lambda, self).__init__(name=name, scope=scope)\n",
        "        self._factory = f\n",
        "\n",
        "    def _forward(self, inputs, **kwargs):\n",
        "        return self._factory(inputs, **kwargs)\n",
        "\n",
        "    def __call__(self, inputs, **kwargs):\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            # Here `reopen_name_scope` is set to True, so that multiple\n",
        "            # calls to the same Module instance will always generate operations\n",
        "            # within the original name scope.\n",
        "            # However, in order for ``tf.variable_scope(default_name=...)``\n",
        "            # to work properly with variable reusing, we must generate a nested\n",
        "            # unique name scope.\n",
        "            with tf.name_scope('forward'):\n",
        "                return self._forward(inputs, **kwargs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Y3xPtBU9vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "34d0c27b-5d08-45b7-f684-90e4b3f64d73"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tfsnippet.distributions import Distribution\n",
        "\n",
        "\n",
        "class TfpDistribution(Distribution):\n",
        "    \"\"\"\n",
        "    A wrapper class for `tfp.distributions.Distribution`\n",
        "    \"\"\"\n",
        "\n",
        "    @property\n",
        "    def is_continuous(self):\n",
        "        return self._is_continuous\n",
        "\n",
        "    def __init__(self, distribution):\n",
        "        if not isinstance(distribution, tfp.distributions.Distribution):\n",
        "            raise TypeError('`distribution` is not an instance of `tfp.'\n",
        "                            'distributions.Distribution`')\n",
        "        super(TfpDistribution, self).__init__()\n",
        "        self._distribution = distribution\n",
        "        self._is_continuous = True\n",
        "        self._is_reparameterized = self._distribution.reparameterization_type is tfp.distributions.FULLY_REPARAMETERIZED\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Distribution({!r})'.format(self._distribution)\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self._distribution.dtype\n",
        "\n",
        "    @property\n",
        "    def is_reparameterized(self):\n",
        "        return self._is_reparameterized\n",
        "\n",
        "    @property\n",
        "    def value_shape(self):\n",
        "        return self._distribution.event_shape\n",
        "\n",
        "    def get_value_shape(self):\n",
        "        return self._distribution.event_shape\n",
        "\n",
        "    @property\n",
        "    def batch_shape(self):\n",
        "        return self._distribution.batch_shape\n",
        "\n",
        "    def get_batch_shape(self):\n",
        "        return self._distribution.batch_shape()\n",
        "\n",
        "    def sample(self, n_samples=None, is_reparameterized=None, group_ndims=0, compute_density=False,\n",
        "               name=None):\n",
        "        from tfsnippet.stochastic import StochasticTensor\n",
        "        if n_samples is None or n_samples < 2:\n",
        "            n_samples = 2\n",
        "        with tf.name_scope(name=name, default_name='sample'):\n",
        "            samples = self._distribution.sample(n_samples)\n",
        "            samples = tf.reduce_mean(samples, axis=0)\n",
        "            t = StochasticTensor(\n",
        "                distribution=self,\n",
        "                tensor=samples,\n",
        "                n_samples=n_samples,\n",
        "                group_ndims=group_ndims,\n",
        "                is_reparameterized=self.is_reparameterized\n",
        "            )\n",
        "            if compute_density:\n",
        "                with tf.name_scope('compute_prob_and_log_prob'):\n",
        "                    log_p = t.log_prob()\n",
        "                    t._self_prob = tf.exp(log_p)\n",
        "            return t\n",
        "\n",
        "    def log_prob(self, given, group_ndims=0, name=None):\n",
        "        with tf.name_scope(name=name, default_name='log_prob'):\n",
        "            log_prob, _, _, _, _, _, _ = self._distribution.forward_filter(given)\n",
        "            return log_prob\n",
        "\n",
        "\n",
        "def softplus_std(inputs, units, epsilon, name):\n",
        "    return tf.nn.softplus(tf.layers.dense(inputs, units, name=name, reuse=tf.AUTO_REUSE)) + epsilon\n",
        "\n",
        "\n",
        "def rnn(x,\n",
        "        window_length,\n",
        "        rnn_num_hidden,\n",
        "        rnn_cell='GRU',\n",
        "        hidden_dense=2,\n",
        "        dense_dim=200,\n",
        "        time_axis=1,\n",
        "        name='rnn'):\n",
        "    from tensorflow.contrib import rnn\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        if len(x.shape) == 4:\n",
        "            x = tf.reduce_mean(x, axis=0)\n",
        "        elif len(x.shape) != 3:\n",
        "            logging.error(\"rnn input shape error\")\n",
        "        x = tf.unstack(x, window_length, time_axis)\n",
        "\n",
        "        if rnn_cell == 'LSTM':\n",
        "            # Define lstm cells with TensorFlow\n",
        "            # Forward direction cell\n",
        "            fw_cell = rnn.BasicLSTMCell(rnn_num_hidden,\n",
        "                                        forget_bias=1.0)\n",
        "        elif rnn_cell == \"GRU\":\n",
        "            fw_cell = tf.nn.rnn_cell.GRUCell(rnn_num_hidden)\n",
        "        elif rnn_cell == 'Basic':\n",
        "            fw_cell = tf.nn.rnn_cell.BasicRNNCell(rnn_num_hidden)\n",
        "        else:\n",
        "            raise ValueError(\"rnn_cell must be LSTM or GRU\")\n",
        "\n",
        "        # Get lstm cell output\n",
        "\n",
        "        try:\n",
        "            outputs, _ = rnn.static_rnn(fw_cell, x, dtype=tf.float32)\n",
        "        except Exception:  # Old TensorFlow version only returns outputs not states\n",
        "            outputs = rnn.static_rnn(fw_cell, x, dtype=tf.float32)\n",
        "        outputs = tf.stack(outputs, axis=time_axis)\n",
        "        for i in range(hidden_dense):\n",
        "            outputs = tf.layers.dense(outputs, dense_dim)\n",
        "        return outputs\n",
        "    # return size: (batch_size, window_length, rnn_num_hidden)\n",
        "\n",
        "\n",
        "def wrap_params_net(inputs, h_for_dist, mean_layer, std_layer):\n",
        "    with tf.variable_scope('hidden', reuse=tf.AUTO_REUSE):\n",
        "        h = h_for_dist(inputs)\n",
        "    return {\n",
        "        'mean': mean_layer(h),\n",
        "        'std': std_layer(h),\n",
        "    }\n",
        "\n",
        "\n",
        "def wrap_params_net_srnn(inputs, h_for_dist):\n",
        "    with tf.variable_scope('hidden', reuse=tf.AUTO_REUSE):\n",
        "        h = h_for_dist(inputs)\n",
        "    return {\n",
        "        'input_q': h\n",
        "    }\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f8855cd960c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfsnippet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCmQA6WmOF8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "c9c39930-1e2c-4a7e-bd12-f0460b9460f9"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "import tfsnippet as spt\n",
        "from tensorflow.python.ops.linalg.linear_operator_identity import LinearOperatorIdentity\n",
        "from tensorflow_probability.python.distributions import LinearGaussianStateSpaceModel, MultivariateNormalDiag\n",
        "from tfsnippet.distributions import Normal\n",
        "from tfsnippet.utils import VarScopeObject, reopen_variable_scope\n",
        "from tfsnippet.variational import VariationalInference\n",
        "\n",
        "from omni_anomaly.recurrent_distribution import RecurrentDistribution\n",
        "from omni_anomaly.vae import Lambda, VAE\n",
        "from omni_anomaly.wrapper import TfpDistribution, softplus_std, rnn, wrap_params_net\n",
        "\n",
        "\n",
        "class OmniAnomaly(VarScopeObject):\n",
        "    def __init__(self, config, name=None, scope=None):\n",
        "        self.config = config\n",
        "        super(OmniAnomaly, self).__init__(name=name, scope=scope)\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            if config.posterior_flow_type == 'nf':\n",
        "                self._posterior_flow = spt.layers.planar_normalizing_flows(\n",
        "                    config.nf_layers, name='posterior_flow')\n",
        "            else:\n",
        "                self._posterior_flow = None\n",
        "            self._window_length = config.window_length\n",
        "            self._x_dims = config.x_dim\n",
        "            self._z_dims = config.z_dim\n",
        "            self._vae = VAE(\n",
        "                p_z=TfpDistribution(\n",
        "                    LinearGaussianStateSpaceModel(\n",
        "                        num_timesteps=config.window_length,\n",
        "                        transition_matrix=LinearOperatorIdentity(config.z_dim),\n",
        "                        transition_noise=MultivariateNormalDiag(\n",
        "                            scale_diag=tf.ones([config.z_dim])),\n",
        "                        observation_matrix=LinearOperatorIdentity(config.z_dim),\n",
        "                        observation_noise=MultivariateNormalDiag(\n",
        "                            scale_diag=tf.ones([config.z_dim])),\n",
        "                        initial_state_prior=MultivariateNormalDiag(\n",
        "                            scale_diag=tf.ones([config.z_dim]))\n",
        "                    )\n",
        "                ) if config.use_connected_z_p else Normal(mean=tf.zeros([config.z_dim]), std=tf.ones([config.z_dim])),\n",
        "                p_x_given_z=Normal,\n",
        "                q_z_given_x=partial(RecurrentDistribution,\n",
        "                                    mean_q_mlp=partial(tf.layers.dense, units=config.z_dim, name='z_mean', reuse=tf.AUTO_REUSE),\n",
        "                                    std_q_mlp=partial(softplus_std, units=config.z_dim, epsilon=config.std_epsilon,\n",
        "                                                      name='z_std'),\n",
        "                                    z_dim=config.z_dim, window_length=config.window_length) if config.use_connected_z_q else Normal,\n",
        "                h_for_p_x=Lambda(\n",
        "                    partial(\n",
        "                        wrap_params_net,\n",
        "                        h_for_dist=lambda x: rnn(x=x,\n",
        "                                                 window_length=config.window_length,\n",
        "                                                 rnn_num_hidden=config.rnn_num_hidden,\n",
        "                                                 hidden_dense=2,\n",
        "                                                 dense_dim=config.dense_dim,\n",
        "                                                 name='rnn_p_x'),\n",
        "                        mean_layer=partial(\n",
        "                            tf.layers.dense, units=config.x_dim, name='x_mean', reuse=tf.AUTO_REUSE\n",
        "                        ),\n",
        "                        std_layer=partial(\n",
        "                            softplus_std, units=config.x_dim, epsilon=config.std_epsilon,\n",
        "                            name='x_std'\n",
        "                        )\n",
        "                    ),\n",
        "                    name='p_x_given_z'\n",
        "                ),\n",
        "                h_for_q_z=Lambda(\n",
        "                    lambda x: {'input_q': rnn(x=x,\n",
        "                                              window_length=config.window_length,\n",
        "                                              rnn_num_hidden=config.rnn_num_hidden,\n",
        "                                              hidden_dense=2,\n",
        "                                              dense_dim=config.dense_dim,\n",
        "                                              name=\"rnn_q_z\")},\n",
        "                    name='q_z_given_x'\n",
        "                ) if config.use_connected_z_q else Lambda(\n",
        "                    partial(\n",
        "                        wrap_params_net,\n",
        "                        h_for_dist=lambda x: rnn(x=x,\n",
        "                                                 window_length=config.window_length,\n",
        "                                                 rnn_num_hidden=config.rnn_num_hidden,\n",
        "                                                 hidden_dense=2,\n",
        "                                                 dense_dim=config.dense_dim,\n",
        "                                                 name=\"rnn_q_z\"),\n",
        "                        mean_layer=partial(\n",
        "                            tf.layers.dense, units=config.z_dim, name='z_mean', reuse=tf.AUTO_REUSE\n",
        "                        ),\n",
        "                        std_layer=partial(\n",
        "                            softplus_std, units=config.z_dim, epsilon=config.std_epsilon,\n",
        "                            name='z_std'\n",
        "                        )\n",
        "                    ),\n",
        "                    name='q_z_given_x'\n",
        "                )\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def x_dims(self):\n",
        "        \"\"\"Get the number of `x` dimensions.\"\"\"\n",
        "        return self._x_dims\n",
        "\n",
        "    @property\n",
        "    def z_dims(self):\n",
        "        \"\"\"Get the number of `z` dimensions.\"\"\"\n",
        "        return self._z_dims\n",
        "\n",
        "    @property\n",
        "    def vae(self):\n",
        "        \"\"\"\n",
        "        Get the VAE object of this :class:`OmniAnomaly` model.\n",
        "\n",
        "        Returns:\n",
        "            VAE: The VAE object of this model.\n",
        "        \"\"\"\n",
        "        return self._vae\n",
        "\n",
        "    @property\n",
        "    def window_length(self):\n",
        "        return self._window_length\n",
        "\n",
        "    def get_training_loss(self, x, n_z=None):\n",
        "        \"\"\"\n",
        "        Get the training loss for `x`.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
        "                KPI observations in a mini-batch.\n",
        "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "                (default :obj:`None`, one sample without explicit sampling\n",
        "                dimension)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: 0-d tensor, the training loss, which can be optimized\n",
        "                by gradient descent algorithms.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('training_loss'):\n",
        "            chain = self.vae.chain(x, n_z=n_z, posterior_flow=self._posterior_flow)\n",
        "            x_log_prob = chain.model['x'].log_prob(group_ndims=0)\n",
        "            log_joint = tf.reduce_sum(x_log_prob, -1)\n",
        "            chain.vi.training.sgvb()\n",
        "            vi = VariationalInference(\n",
        "                log_joint=log_joint,\n",
        "                latent_log_probs=chain.vi.latent_log_probs,\n",
        "                axis=chain.vi.axis\n",
        "            )\n",
        "            loss = tf.reduce_mean(vi.training.sgvb())\n",
        "            return loss\n",
        "\n",
        "    def get_score(self, x, n_z=None,\n",
        "                  last_point_only=True):\n",
        "        \"\"\"\n",
        "        Get the reconstruction probability for `x`.\n",
        "\n",
        "        The larger `reconstruction probability`, the less likely a point\n",
        "        is anomaly.  You may take the negative of the score, if you want\n",
        "        something to directly indicate the severity of anomaly.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
        "                KPI observations in a mini-batch.\n",
        "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "                (default :obj:`None`, one sample without explicit sampling\n",
        "                dimension)\n",
        "            last_point_only (bool): Whether to obtain the reconstruction\n",
        "                probability of only the last point in each window?\n",
        "                (default :obj:`True`)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The reconstruction probability, with the shape\n",
        "                ``(len(x) - self.x_dims + 1,)`` if `last_point_only` is\n",
        "                :obj:`True`, or ``(len(x) - self.x_dims + 1, self.x_dims)``\n",
        "                if `last_point_only` is :obj:`False`.  This is because the\n",
        "                first ``self.x_dims - 1`` points are not the last point of\n",
        "                any window.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('get_score'):\n",
        "            x_r = x\n",
        "\n",
        "            # get the reconstruction probability\n",
        "            print('-' * 30, 'testing', '-' * 30)\n",
        "            q_net = self.vae.variational(x=x_r, n_z=n_z, posterior_flow=self._posterior_flow)  # notice: x=x_r\n",
        "            p_net = self.vae.model(z=q_net['z'], x=x, n_z=n_z)  # notice: x=x\n",
        "            z_samples = q_net['z'].tensor\n",
        "            z_mean = tf.reduce_mean(z_samples, axis=0) if n_z is not None else z_samples\n",
        "            z_std = tf.sqrt(tf.reduce_sum(tf.square(z_samples - z_mean), axis=0) / (n_z - 1)) \\\n",
        "                if n_z is not None and n_z > 1 else tf.zeros_like(z_mean)\n",
        "            z = tf.concat((z_mean, z_std), axis=-1)\n",
        "\n",
        "            r_prob = p_net['x'].log_prob(group_ndims=int(not self.config.get_score_on_dim))\n",
        "\n",
        "            if last_point_only:\n",
        "                r_prob = r_prob[:, -1]\n",
        "            return r_prob, z\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7b620e1bf49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtfsnippet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_operator_identity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperatorIdentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1jvxLUePwbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "? pip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}