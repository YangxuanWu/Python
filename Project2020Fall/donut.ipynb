{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "donut.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kMsRnpZ0HbMA",
        "Wv6xw_SpHbMI",
        "URff2TPIHbMY",
        "MJnNVrRnHbNa",
        "Tzf41HvLHbNd",
        "oJSERQxiHbNk",
        "ddXz1WmCHbNl",
        "8PeM9TcuHbOC"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangxuanWu/Python/blob/master/Project2020Fall/donut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMnIrU2AHbLg",
        "colab_type": "text"
      },
      "source": [
        "#donut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyxBYCKL6agU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd443ea1-add5-4ee9-e882-83364b46f614"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/NetManAIOps/\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QuUNGYTe3ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c560876e-a648-48ca-c0a3-cd826745cf51"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu4.csv     g.csv\t\t\t      server_res_eth1out_curve_6.csv\n",
            "donut.ipynb  server_res_eth1out_curve_61.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGLS5mFByj3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b2cbea1f-3b88-4a09-958b-b55a545197ad"
      },
      "source": [
        "!pip install git+https://github.com/thu-ml/zhusuan.git"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/thu-ml/zhusuan.git\n",
            "  Cloning https://github.com/thu-ml/zhusuan.git to /tmp/pip-req-build-5lktt18u\n",
            "  Running command git clone -q https://github.com/thu-ml/zhusuan.git /tmp/pip-req-build-5lktt18u\n",
            "Requirement already satisfied (use --upgrade to upgrade): zhusuan==0.4.0 from git+https://github.com/thu-ml/zhusuan.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from zhusuan==0.4.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from zhusuan==0.4.0) (1.15.0)\n",
            "Building wheels for collected packages: zhusuan\n",
            "  Building wheel for zhusuan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zhusuan: filename=zhusuan-0.4.0-py2.py3-none-any.whl size=73591 sha256=8cb7252e565b11927f5d45a1ac289ab12297d210be90765d29411aabe683329e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ev4pbop/wheels/45/cb/f5/bfd913ae94924c3151ac7d20dab61be39e90a2b07bdc6cb75e\n",
            "Successfully built zhusuan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDY5k8eRyrLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "8c32ed91-2ae9-4f98-92e9-711bcc32bc9a"
      },
      "source": [
        "!pip install git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2\n",
            "  Cloning https://github.com/haowen-xu/tfsnippet.git (to revision v0.1.2) to /tmp/pip-req-build-ttz48_w4\n",
            "  Running command git clone -q https://github.com/haowen-xu/tfsnippet.git /tmp/pip-req-build-ttz48_w4\n",
            "  Running command git checkout -q ecc0b4d1e610cf8cfa8c236857a7dabee27d5543\n",
            "Requirement already satisfied (use --upgrade to upgrade): TFSnippet==0.1.2 from git+https://github.com/haowen-xu/tfsnippet.git@v0.1.2 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: filelock>=3.0.10 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (3.0.12)\n",
            "Requirement already satisfied: frozendict>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (1.2)\n",
            "Requirement already satisfied: idx2numpy>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (1.2.2)\n",
            "Requirement already satisfied: natsort>=5.3.3 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (2.23.0)\n",
            "Requirement already satisfied: semver>=2.7.9 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (2.10.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.6/dist-packages (from TFSnippet==0.1.2) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.1.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.1.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.1.2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->TFSnippet==0.1.2) (1.24.3)\n",
            "Building wheels for collected packages: TFSnippet\n",
            "  Building wheel for TFSnippet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TFSnippet: filename=TFSnippet-0.1.2-cp36-none-any.whl size=166293 sha256=69ae22dd19a5bd4cfb5e4691cb248b549a6862e4b1104789aa7d1ec0500a07a2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2l3m7vwo/wheels/96/11/7f/b481019e227199360434b1344f5468eb77d69e838f0e0f040a\n",
            "Successfully built TFSnippet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWEDJrtHgVxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2dcd3d5-1538-4636-8bbb-55f4ef281bad"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_S7bNylg3TA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "30e22d95-a6b7-4277-d441-a32755608219"
      },
      "source": [
        "!pip install tensorflow==1.5"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.5 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-tensorboard<1.6.0,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.5.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.34.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.0.1)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (0.9999999)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.5) (49.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHXzWXONyzir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction.py\n",
        "import six\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tfsnippet.utils import (VarScopeObject, get_default_session_or_error,\n",
        "                             reopen_variable_scope)\n",
        "\n",
        "###from .model import Donut\n",
        "###from .utils import BatchSlidingWindow\n",
        "\n",
        "###__all__ = ['DonutPredictor']\n",
        "\n",
        "\n",
        "class DonutPredictor(VarScopeObject):\n",
        "    \"\"\"\n",
        "    Donut predictor.\n",
        "    Args:\n",
        "        model (Donut): The :class:`Donut` model instance.\n",
        "        n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "            If :obj:`None`, one sample without explicit sampling dimension.\n",
        "            (default 1024)\n",
        "        mcmc_iteration: (int or tf.Tensor): Iteration count for MCMC\n",
        "            missing data imputation. (default 10)\n",
        "        batch_size (int): Size of each mini-batch for prediction.\n",
        "            (default 32)\n",
        "        feed_dict (dict[tf.Tensor, any]): User provided feed dict for\n",
        "            prediction. (default :obj:`None`)\n",
        "        last_point_only (bool): Whether to obtain the reconstruction\n",
        "            probability of only the last point in each window?\n",
        "            (default :obj:`True`)\n",
        "        name (str): Optional name of this predictor\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "        scope (str): Optional scope of this predictor\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, n_z=1024, mcmc_iteration=10, batch_size=32,\n",
        "                 feed_dict=None, last_point_only=True, name=None, scope=None):\n",
        "        super(DonutPredictor, self).__init__(name=name, scope=scope)\n",
        "        self._model = model\n",
        "        self._n_z = n_z\n",
        "        self._mcmc_iteration = mcmc_iteration\n",
        "        self._batch_size = batch_size\n",
        "        if feed_dict is not None:\n",
        "            self._feed_dict = dict(six.iteritems(feed_dict))\n",
        "        else:\n",
        "            self._feed_dict = {}\n",
        "        self._last_point_only = last_point_only\n",
        "\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            # input placeholders\n",
        "            self._input_x = tf.placeholder(\n",
        "                dtype=tf.float32, shape=[None, model.x_dims], name='input_x')\n",
        "            self._input_y = tf.placeholder(\n",
        "                dtype=tf.int32, shape=[None, model.x_dims], name='input_y')\n",
        "\n",
        "            # outputs of interest\n",
        "            self._score = self._score_without_y = None\n",
        "\n",
        "    def _get_score(self):\n",
        "        if self._score is None:\n",
        "            with reopen_variable_scope(self.variable_scope), \\\n",
        "                    tf.name_scope('score'):\n",
        "                self._score = self.model.get_score(\n",
        "                    x=self._input_x,\n",
        "                    y=self._input_y,\n",
        "                    n_z=self._n_z,\n",
        "                    mcmc_iteration=self._mcmc_iteration,\n",
        "                    last_point_only=self._last_point_only\n",
        "                )\n",
        "        return self._score\n",
        "\n",
        "    def _get_score_without_y(self):\n",
        "        if self._score_without_y is None:\n",
        "            with reopen_variable_scope(self.variable_scope), \\\n",
        "                    tf.name_scope('score_without_y'):\n",
        "                self._score_without_y = self.model.get_score(\n",
        "                    x=self._input_x,\n",
        "                    n_z=self._n_z,\n",
        "                    last_point_only=self._last_point_only\n",
        "                )\n",
        "        return self._score_without_y\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"\n",
        "        Get the :class:`Donut` model instance.\n",
        "        Returns:\n",
        "            Donut: The :class:`Donut` model instance.\n",
        "        \"\"\"\n",
        "        return self._model\n",
        "\n",
        "    def get_score(self, values, missing=None):\n",
        "        \"\"\"\n",
        "        Get the `reconstruction probability` of specified KPI observations.\n",
        "        The larger `reconstruction probability`, the less likely a point\n",
        "        is anomaly.  You may take the negative of the score, if you want\n",
        "        something to directly indicate the severity of anomaly.\n",
        "        Args:\n",
        "            values (np.ndarray): 1-D float32 array, the KPI observations.\n",
        "            missing (np.ndarray): 1-D int32 array, the indicator of missing\n",
        "                points.  If :obj:`None`, the MCMC missing data imputation\n",
        "                will be disabled. (default :obj:`None`)\n",
        "        Returns:\n",
        "            np.ndarray: The `reconstruction probability`,\n",
        "                1-D array if `last_point_only` is :obj:`True`,\n",
        "                or 2-D array if `last_point_only` is :obj:`False`.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('DonutPredictor.get_score'):\n",
        "            sess = get_default_session_or_error()\n",
        "            collector = []\n",
        "\n",
        "            # validate the arguments\n",
        "            values = np.asarray(values, dtype=np.float32)\n",
        "            if len(values.shape) != 1:\n",
        "                raise ValueError('`values` must be a 1-D array')\n",
        "\n",
        "            # run the prediction in mini-batches\n",
        "            sliding_window = BatchSlidingWindow(\n",
        "                array_size=len(values),\n",
        "                window_size=self.model.x_dims,\n",
        "                batch_size=self._batch_size,\n",
        "            )\n",
        "            if missing is not None:\n",
        "                missing = np.asarray(missing, dtype=np.int32)\n",
        "                if missing.shape != values.shape:\n",
        "                    raise ValueError('The shape of `missing` does not agree '\n",
        "                                     'with the shape of `values` ({} vs {})'.\n",
        "                                     format(missing.shape, values.shape))\n",
        "                for b_x, b_y in sliding_window.get_iterator([values, missing]):\n",
        "                    feed_dict = dict(six.iteritems(self._feed_dict))\n",
        "                    feed_dict[self._input_x] = b_x\n",
        "                    feed_dict[self._input_y] = b_y\n",
        "                    b_r = sess.run(self._get_score(), feed_dict=feed_dict)\n",
        "                    collector.append(b_r)\n",
        "            else:\n",
        "                for b_x, in sliding_window.get_iterator([values]):\n",
        "                    feed_dict = dict(six.iteritems(self._feed_dict))\n",
        "                    feed_dict[self._input_x] = b_x\n",
        "                    b_r = sess.run(self._get_score_without_y(),\n",
        "                                   feed_dict=feed_dict)\n",
        "                    collector.append(b_r)\n",
        "\n",
        "            # merge the results of mini-batches\n",
        "            result = np.concatenate(collector, axis=0)\n",
        "            return result"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kUOhVDEHbLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b972fe7-aa35-4c37-b0f9-01ec36de0151"
      },
      "source": [
        "#model.py\n",
        "import warnings\n",
        "from functools import partial\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfsnippet.distributions import Normal\n",
        "from tfsnippet.modules import VAE, Lambda, Module\n",
        "from tfsnippet.stochastic import validate_n_samples\n",
        "from tfsnippet.utils import (VarScopeObject,\n",
        "                             reopen_variable_scope,\n",
        "                             is_integer)\n",
        "from tfsnippet.variational import VariationalInference\n",
        "\n",
        "###from .reconstruction import iterative_masked_reconstruct\n",
        "\n",
        "###__all__ = ['Donut']\n",
        "\n",
        "def softplus_std(inputs, units, epsilon, name):\n",
        "    return tf.nn.softplus(tf.layers.dense(inputs, units, name=name)) + epsilon\n",
        "\n",
        "\n",
        "def wrap_params_net(inputs, h_for_dist, mean_layer, std_layer):\n",
        "    with tf.variable_scope('hidden'):\n",
        "        h = h_for_dist(inputs)\n",
        "    return {\n",
        "        'mean': mean_layer(h),\n",
        "        'std': std_layer(h),\n",
        "    }\n",
        "\n",
        "\n",
        "class Donut(VarScopeObject):\n",
        "    \"\"\"\n",
        "    Class for constructing Donut model.\n",
        "    This class provides :meth:`get_training_loss` for deriving the\n",
        "    training loss :class:`tf.Tensor`, and :meth:`get_score` for obtaining\n",
        "    the reconstruction probability :class:`tf.Tensor`.\n",
        "    Note:\n",
        "        :class:`Donut` instances will not build the computation graph\n",
        "        until :meth:`get_training_loss` or :meth:`get_score` is\n",
        "        called.  This suggests that a :class:`donut.DonutTrainer` or\n",
        "        a :class:`donut.DonutPredictor` must have been constructed\n",
        "        before saving or restoring the model parameters.\n",
        "    Args:\n",
        "        h_for_p_x (Module or (tf.Tensor) -> tf.Tensor):\n",
        "            The hidden network for :math:`p(x|z)`.\n",
        "        h_for_q_z (Module or (tf.Tensor) -> tf.Tensor):\n",
        "            The hidden network for :math:`q(z|x)`.\n",
        "        x_dims (int): The number of `x` dimensions.\n",
        "        z_dims (int): The number of `z` dimensions.\n",
        "        std_epsilon (float): The minimum value of std for `x` and `z`.\n",
        "        name (str): Optional name of this module\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "        scope (str): Optional scope of this module\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "    \"\"\"\n",
        "    def __init__(self, h_for_p_x, h_for_q_z, x_dims, z_dims, std_epsilon=1e-4,\n",
        "                 name=None, scope=None):\n",
        "        if not is_integer(x_dims) or x_dims <= 0:\n",
        "            raise ValueError('`x_dims` must be a positive integer')\n",
        "        if not is_integer(z_dims) or z_dims <= 0:\n",
        "            raise ValueError('`z_dims` must be a positive integer')\n",
        "\n",
        "        super(Donut, self).__init__(name=name, scope=scope)\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            self._vae = VAE(\n",
        "                p_z=Normal(mean=tf.zeros([z_dims]), std=tf.ones([z_dims])),\n",
        "                p_x_given_z=Normal,\n",
        "                q_z_given_x=Normal,\n",
        "                h_for_p_x=Lambda(\n",
        "                    partial(\n",
        "                        wrap_params_net,\n",
        "                        h_for_dist=h_for_p_x,\n",
        "                        mean_layer=partial(\n",
        "                            tf.layers.dense, units=x_dims, name='x_mean'\n",
        "                        ),\n",
        "                        std_layer=partial(\n",
        "                            softplus_std, units=x_dims, epsilon=std_epsilon,\n",
        "                            name='x_std'\n",
        "                        )\n",
        "                    ),\n",
        "                    name='p_x_given_z'\n",
        "                ),\n",
        "                h_for_q_z=Lambda(\n",
        "                    partial(\n",
        "                        wrap_params_net,\n",
        "                        h_for_dist=h_for_q_z,\n",
        "                        mean_layer=partial(\n",
        "                            tf.layers.dense, units=z_dims, name='z_mean'\n",
        "                        ),\n",
        "                        std_layer=partial(\n",
        "                            softplus_std, units=z_dims, epsilon=std_epsilon,\n",
        "                            name='z_std'\n",
        "                        )\n",
        "                    ),\n",
        "                    name='q_z_given_x'\n",
        "                )\n",
        "            )\n",
        "        self._x_dims = x_dims\n",
        "        self._z_dims = z_dims\n",
        "\n",
        "    @property\n",
        "    def x_dims(self):\n",
        "        \"\"\"Get the number of `x` dimensions.\"\"\"\n",
        "        return self._x_dims\n",
        "\n",
        "    @property\n",
        "    def z_dims(self):\n",
        "        \"\"\"Get the number of `z` dimensions.\"\"\"\n",
        "        return self._z_dims\n",
        "\n",
        "    @property\n",
        "    def vae(self):\n",
        "        \"\"\"\n",
        "        Get the VAE object of this :class:`Donut` model.\n",
        "        Returns:\n",
        "            VAE: The VAE object of this model.\n",
        "        \"\"\"\n",
        "        return self._vae\n",
        "\n",
        "    def get_training_loss(self, x, y, n_z=None):\n",
        "        \"\"\"\n",
        "        Get the training loss for `x` and `y`.\n",
        "        Args:\n",
        "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
        "                KPI observations in a mini-batch.\n",
        "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
        "                ``(label | missing)`` in a mini-batch.\n",
        "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "                (default :obj:`None`, one sample without explicit sampling\n",
        "                dimension)\n",
        "        Returns:\n",
        "            tf.Tensor: 0-d tensor, the training loss, which can be optimized\n",
        "                by gradient descent algorithms.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('Donut.training_loss'):\n",
        "            chain = self.vae.chain(x, n_z=n_z)\n",
        "            x_log_prob = chain.model['x'].log_prob(group_ndims=0)\n",
        "            alpha = tf.cast(1 - y, dtype=tf.float32)\n",
        "            beta = tf.reduce_mean(alpha, axis=-1)\n",
        "            log_joint = (\n",
        "                tf.reduce_sum(alpha * x_log_prob, axis=-1) +\n",
        "                beta * chain.model['z'].log_prob()\n",
        "            )\n",
        "            vi = VariationalInference(\n",
        "                log_joint=log_joint,\n",
        "                latent_log_probs=chain.vi.latent_log_probs,\n",
        "                axis=chain.vi.axis\n",
        "            )\n",
        "            loss = tf.reduce_mean(vi.training.sgvb())\n",
        "            return loss\n",
        "\n",
        "    def get_training_objective(self, *args, **kwargs):  # pragma: no cover\n",
        "        warnings.warn('`get_training_objective` is deprecated, use '\n",
        "                      '`get_training_loss` instead.', DeprecationWarning)\n",
        "        return self.get_training_loss(*args, **kwargs)\n",
        "\n",
        "    def get_score(self, x, y=None, n_z=None, mcmc_iteration=None,\n",
        "                  last_point_only=True):\n",
        "        \"\"\"\n",
        "        Get the reconstruction probability for `x` and `y`.\n",
        "        The larger `reconstruction probability`, the less likely a point\n",
        "        is anomaly.  You may take the negative of the score, if you want\n",
        "        something to directly indicate the severity of anomaly.\n",
        "        Args:\n",
        "            x (tf.Tensor): 2-D `float32` :class:`tf.Tensor`, the windows of\n",
        "                KPI observations in a mini-batch.\n",
        "            y (tf.Tensor): 2-D `int32` :class:`tf.Tensor`, the windows of\n",
        "                missing point indicators in a mini-batch.\n",
        "            n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "                (default :obj:`None`, one sample without explicit sampling\n",
        "                dimension)\n",
        "            mcmc_iteration (int or tf.Tensor): Iteration count for MCMC\n",
        "                missing data imputation. (default :obj:`None`, no iteration)\n",
        "            last_point_only (bool): Whether to obtain the reconstruction\n",
        "                probability of only the last point in each window?\n",
        "                (default :obj:`True`)\n",
        "        Returns:\n",
        "            tf.Tensor: The reconstruction probability, with the shape\n",
        "                ``(len(x) - self.x_dims + 1,)`` if `last_point_only` is\n",
        "                :obj:`True`, or ``(len(x) - self.x_dims + 1, self.x_dims)``\n",
        "                if `last_point_only` is :obj:`False`.  This is because the\n",
        "                first ``self.x_dims - 1`` points are not the last point of\n",
        "                any window.\n",
        "        \"\"\"\n",
        "        with tf.name_scope('Donut.get_score'):\n",
        "            # MCMC missing data imputation\n",
        "            if y is not None and mcmc_iteration:\n",
        "                x_r = iterative_masked_reconstruct(\n",
        "                    reconstruct=self.vae.reconstruct,\n",
        "                    x=x,\n",
        "                    mask=y,\n",
        "                    iter_count=mcmc_iteration,\n",
        "                    back_prop=False,\n",
        "                )\n",
        "            else:\n",
        "                x_r = x\n",
        "\n",
        "            # get the reconstruction probability\n",
        "            q_net = self.vae.variational(x=x_r, n_z=n_z)  # notice: x=x_r\n",
        "            p_net = self.vae.model(z=q_net['z'], x=x, n_z=n_z)  # notice: x=x\n",
        "            r_prob = p_net['x'].log_prob(group_ndims=0)\n",
        "            if n_z is not None:\n",
        "                n_z = validate_n_samples(n_z, 'n_z')\n",
        "                assert_shape_op = tf.assert_equal(\n",
        "                    tf.shape(r_prob),\n",
        "                    tf.stack([n_z, tf.shape(x)[0], self.x_dims]),\n",
        "                    message='Unexpected shape of reconstruction prob'\n",
        "                )\n",
        "                with tf.control_dependencies([assert_shape_op]):\n",
        "                    r_prob = tf.reduce_mean(r_prob, axis=0)\n",
        "            if last_point_only:\n",
        "                r_prob = r_prob[:, -1]\n",
        "            return r_prob"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28lLWFg904q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing.py\n",
        "import numpy as np\n",
        "\n",
        "###__all__ = ['complete_timestamp', 'standardize_kpi']\n",
        "\n",
        "\n",
        "def complete_timestamp(timestamp, arrays=None):\n",
        "    \"\"\"\n",
        "    Complete `timestamp` such that the time interval is homogeneous.\n",
        "    Zeros will be inserted into each array in `arrays`, at missing points.\n",
        "    Also, an indicator array will be returned to indicate whether each\n",
        "    point is missing or not.\n",
        "    Args:\n",
        "        timestamp (np.ndarray): 1-D int64 array, the timestamp values.\n",
        "            It can be unsorted.\n",
        "        arrays (Iterable[np.ndarray]): The 1-D arrays to be filled with zeros\n",
        "            according to `timestamp`.\n",
        "    Returns:\n",
        "        np.ndarray: A 1-D int64 array, the completed timestamp.\n",
        "        np.ndarray: A 1-D int32 array, indicating whether each point is missing.\n",
        "        list[np.ndarray]: The arrays, missing points filled with zeros.\n",
        "            (optional, return only if `arrays` is specified)\n",
        "    \"\"\"\n",
        "    timestamp = np.asarray(timestamp, np.int64)\n",
        "    if len(timestamp.shape) != 1:\n",
        "        raise ValueError('`timestamp` must be a 1-D array')\n",
        "\n",
        "    has_arrays = arrays is not None\n",
        "    arrays = [np.asarray(array) for array in (arrays or ())]\n",
        "    for i, array in enumerate(arrays):\n",
        "        if array.shape != timestamp.shape:\n",
        "            raise ValueError('The shape of ``arrays[{}]`` does not agree with '\n",
        "                             'the shape of `timestamp` ({} vs {})'.\n",
        "                             format(i, array.shape, timestamp.shape))\n",
        "\n",
        "    # sort the timestamp, and check the intervals\n",
        "    src_index = np.argsort(timestamp)\n",
        "    timestamp_sorted = timestamp[src_index]\n",
        "    intervals = np.unique(np.diff(timestamp_sorted))\n",
        "    interval = np.min(intervals)\n",
        "    if interval == 0:\n",
        "        raise ValueError('Duplicated values in `timestamp`')\n",
        "    for itv in intervals:\n",
        "        if itv % interval != 0:\n",
        "            raise ValueError('Not all intervals in `timestamp` are multiples '\n",
        "                             'of the minimum interval')\n",
        "\n",
        "    # prepare for the return arrays\n",
        "    length = (timestamp_sorted[-1] - timestamp_sorted[0]) // interval + 1\n",
        "    ret_timestamp = np.arange(timestamp_sorted[0],\n",
        "                              timestamp_sorted[-1] + interval,\n",
        "                              interval,\n",
        "                              dtype=np.int64)\n",
        "    ret_missing = np.ones([length], dtype=np.int32)\n",
        "    ret_arrays = [np.zeros([length], dtype=array.dtype) for array in arrays]\n",
        "\n",
        "    # copy values to the return arrays\n",
        "    dst_index = np.asarray((timestamp_sorted - timestamp_sorted[0]) // interval,\n",
        "                           dtype=np.int)\n",
        "    ret_missing[dst_index] = 0\n",
        "    for ret_array, array in zip(ret_arrays, arrays):\n",
        "        ret_array[dst_index] = array[src_index]\n",
        "\n",
        "    if has_arrays:\n",
        "        return ret_timestamp, ret_missing, ret_arrays\n",
        "    else:\n",
        "        return ret_timestamp, ret_missing\n",
        "\n",
        "\n",
        "def standardize_kpi(values, mean=None, std=None, excludes=None):\n",
        "    \"\"\"\n",
        "    Standardize a\n",
        "    Args:\n",
        "        values (np.ndarray): 1-D `float32` array, the KPI observations.\n",
        "        mean (float): If not :obj:`None`, will use this `mean` to standardize\n",
        "            `values`. If :obj:`None`, `mean` will be computed from `values`.\n",
        "            Note `mean` and `std` must be both :obj:`None` or not :obj:`None`.\n",
        "            (default :obj:`None`)\n",
        "        std (float): If not :obj:`None`, will use this `std` to standardize\n",
        "            `values`. If :obj:`None`, `std` will be computed from `values`.\n",
        "            Note `mean` and `std` must be both :obj:`None` or not :obj:`None`.\n",
        "            (default :obj:`None`)\n",
        "        excludes (np.ndarray): Optional, 1-D `int32` or `bool` array, the\n",
        "            indicators of whether each point should be excluded for computing\n",
        "            `mean` and `std`. Ignored if `mean` and `std` are not :obj:`None`.\n",
        "            (default :obj:`None`)\n",
        "    Returns:\n",
        "        np.ndarray: The standardized `values`.\n",
        "        float: The computed `mean` or the given `mean`.\n",
        "        float: The computed `std` or the given `std`.\n",
        "    \"\"\"\n",
        "    values = np.asarray(values, dtype=np.float32)\n",
        "    if len(values.shape) != 1:\n",
        "        raise ValueError('`values` must be a 1-D array')\n",
        "    if (mean is None) != (std is None):\n",
        "        raise ValueError('`mean` and `std` must be both None or not None')\n",
        "    if excludes is not None:\n",
        "        excludes = np.asarray(excludes, dtype=np.bool)\n",
        "        if excludes.shape != values.shape:\n",
        "            raise ValueError('The shape of `excludes` does not agree with '\n",
        "                             'the shape of `values` ({} vs {})'.\n",
        "                             format(excludes.shape, values.shape))\n",
        "\n",
        "    if mean is None:\n",
        "        if excludes is not None:\n",
        "            val = values[np.logical_not(excludes)]\n",
        "        else:\n",
        "            val = values\n",
        "        mean = val.mean()\n",
        "        std = val.std()\n",
        "\n",
        "    return (values - mean) / std, mean, std"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cE3Dzkl1Ebm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reconstruction.py \n",
        "import tensorflow as tf\n",
        "from tfsnippet.utils import TensorArgValidator\n",
        "\n",
        "###__all__ = ['masked_reconstruct', 'iterative_masked_reconstruct']\n",
        "\n",
        "\n",
        "def masked_reconstruct(reconstruct, x, mask, validate_shape=True, name=None):\n",
        "    \"\"\"\n",
        "    Replace masked elements of `x` with reconstructed outputs.\n",
        "    This method can be used to do missing data imputation on `x`, with\n",
        "    the reconstruction outputs for `x`.\n",
        "    Args:\n",
        "        reconstruct ((tf.Tensor) -> tf.Tensor): Function for reconstructing `x`.\n",
        "        x: The tensor to be reconstructed by `func`.\n",
        "        mask: `int32` mask, must be broadcastable into the shape of `x`.\n",
        "            Indicating whether or not to mask each element of `x`.\n",
        "        validate_shape (bool): Whether or not to validate the shape of `mask`?\n",
        "            (default :obj:`True`)\n",
        "        name (str): Name of this operation in TensorFlow graph.\n",
        "            (default \"masked_reconstruct\")\n",
        "    Returns:\n",
        "        tf.Tensor: `x` with masked elements replaced by reconstructed outputs.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, default_name='masked_reconstruct'):\n",
        "        x = tf.convert_to_tensor(x)  # type: tf.Tensor\n",
        "        mask = tf.convert_to_tensor(mask, dtype=tf.int32)  # type: tf.Tensor\n",
        "\n",
        "        # broadcast mask against x\n",
        "        old_mask = mask\n",
        "        try:\n",
        "            _ = tf.broadcast_static_shape(x.get_shape(), mask.get_shape())\n",
        "        except ValueError:\n",
        "            raise ValueError('Shape of `mask` cannot broadcast '\n",
        "                             'into the shape of `x` ({!r} vs {!r})'.\n",
        "                             format(old_mask.get_shape(), x.get_shape()))\n",
        "        mask = mask * tf.ones_like(x, dtype=mask.dtype)\n",
        "\n",
        "        # validate the shape of mask\n",
        "        if validate_shape:\n",
        "            x_shape = x.get_shape()\n",
        "            mask_shape = mask.get_shape()\n",
        "            if mask_shape.is_fully_defined() and x_shape.is_fully_defined():\n",
        "                if mask_shape != x_shape:\n",
        "                    # the only possible situation is that mask has more\n",
        "                    # dimension than x, and we consider this situation invalid\n",
        "                    raise ValueError('Shape of `mask` cannot broadcast '\n",
        "                                     'into the shape of `x` ({!r} vs {!r})'.\n",
        "                                     format(old_mask.get_shape(), x_shape))\n",
        "            else:\n",
        "                assert_op = tf.assert_equal(\n",
        "                    # since already broadcasted by x * ones_like(x),\n",
        "                    # we only need to compare the rank\n",
        "                    tf.rank(x),\n",
        "                    tf.rank(mask),\n",
        "                    message='Shape of `mask` cannot broadcast into the '\n",
        "                            'shape of `x`'\n",
        "                )\n",
        "                with tf.control_dependencies([assert_op]):\n",
        "                    mask = tf.identity(mask)\n",
        "\n",
        "        # get reconstructed x\n",
        "        r_x = reconstruct(x)\n",
        "\n",
        "        # get masked outputs\n",
        "        return tf.where(tf.cast(mask, dtype=tf.bool), r_x, x)\n",
        "\n",
        "\n",
        "def iterative_masked_reconstruct(reconstruct, x, mask, iter_count,\n",
        "                                 back_prop=True, name=None):\n",
        "    \"\"\"\n",
        "    Iteratively reconstruct `x` with `mask` for `iter_count` times.\n",
        "    This method will call :func:`masked_reconstruct` for `iter_count` times,\n",
        "    with the output from previous iteration as the input `x` for the next\n",
        "    iteration.  The output of the final iteration would be returned.\n",
        "    Args:\n",
        "        reconstruct: Function for reconstructing `x`.\n",
        "        x: The tensor to be reconstructed by `func`.\n",
        "        mask: int32 mask, must be broadcastable against `x`.\n",
        "            Indicating whether or not to mask each element of `x`.\n",
        "        iter_count (int or tf.Tensor):\n",
        "            Number of iterations(must be greater than 1).\n",
        "        back_prop (bool): Whether or not to support back-propagation through\n",
        "            all the iterations? (default :obj:`True`)\n",
        "        name (str): Name of this operation in TensorFlow graph.\n",
        "            (default \"iterative_masked_reconstruct\")\n",
        "    Returns:\n",
        "        tf.Tensor: The iteratively reconstructed `x`.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, default_name='iterative_masked_reconstruct'):\n",
        "        # validate the iteration count\n",
        "        v = TensorArgValidator('iter_count')\n",
        "        iter_count = v.require_positive(v.require_int32(iter_count))\n",
        "\n",
        "        # do the masked reconstructions\n",
        "        x_r, _ = tf.while_loop(\n",
        "            lambda x_i, i: i < iter_count,\n",
        "            lambda x_i, i: (masked_reconstruct(reconstruct, x_i, mask), i + 1),\n",
        "            [x, tf.constant(0, dtype=tf.int32)],\n",
        "            back_prop=back_prop\n",
        "        )\n",
        "\n",
        "        return x_r"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6RPUpuU1XGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#augmentation.py\n",
        "import numpy as np\n",
        "from tfsnippet.utils import DocInherit\n",
        "\n",
        "###__all__ = ['DataAugmentation', 'MissingDataInjection']\n",
        "\n",
        "\n",
        "@DocInherit\n",
        "class DataAugmentation(object):\n",
        "    \"\"\"\n",
        "    Base class for data augmentation in training.\n",
        "    Args:\n",
        "        mean (float): Mean of the training data.\n",
        "        std (float): Standard deviation of the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        if std <= 0.:\n",
        "            raise ValueError('`std` must be positive')\n",
        "        self._mean = mean\n",
        "        self._std = std\n",
        "\n",
        "    def augment(self, values, labels, missing):\n",
        "        \"\"\"\n",
        "        Generate augmented data.\n",
        "        Args:\n",
        "            values (np.ndarray): 1-D float32 array of shape `(data_length,)`,\n",
        "                the standardized KPI values.\n",
        "            labels (np.ndarray): 1-D int32 array of shape `(data_length,)`,\n",
        "                the anomaly labels for `values`.\n",
        "            missing (np.ndarray): 1-D int32 array of shape `(data_length,)`,\n",
        "                the indicator of missing points.\n",
        "        Returns:\n",
        "            np.ndarray: The augmented KPI values.\n",
        "            np.ndarray: The augmented labels.\n",
        "            np.ndarray: The augmented indicators of missing points.\n",
        "        \"\"\"\n",
        "        if len(values.shape) != 1:\n",
        "            raise ValueError('`values` must be a 1-D array')\n",
        "        if labels.shape != values.shape:\n",
        "            raise ValueError('The shape of `labels` does not agree with the '\n",
        "                             'shape of `values` ({} vs {})'.\n",
        "                             format(labels.shape, values.shape))\n",
        "        if missing.shape != values.shape:\n",
        "            raise ValueError('The shape of `missing` does not agree with the '\n",
        "                             'shape of `values` ({} vs {})'.\n",
        "                             format(missing.shape, values.shape))\n",
        "        return self._augment(values, labels, missing)\n",
        "\n",
        "    def _augment(self, values, labels, missing):\n",
        "        \"\"\"\n",
        "        Derived classes should override this to actually implement the\n",
        "        data augmentation algorithm.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        \"\"\"Get the mean of the training data.\"\"\"\n",
        "        return self._mean\n",
        "\n",
        "    @property\n",
        "    def std(self):\n",
        "        \"\"\"Get the standard deviation of training data.\"\"\"\n",
        "        return self._std\n",
        "\n",
        "\n",
        "class MissingDataInjection(DataAugmentation):\n",
        "    \"\"\"\n",
        "    Data augmentation by injecting missing points into training data.\n",
        "    Args:\n",
        "        mean (float): Mean of the training data.\n",
        "        std (float): Standard deviation of the training data.\n",
        "        missing_rate (float): The ratio of missing points to inject.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std, missing_rate):\n",
        "        super(MissingDataInjection, self).__init__(mean, std)\n",
        "        self._missing_rate = missing_rate\n",
        "\n",
        "    @property\n",
        "    def missing_rate(self):\n",
        "        \"\"\"Get the ratio of missing points to inject.\"\"\"\n",
        "        return self._missing_rate\n",
        "\n",
        "    def _augment(self, values, labels, missing):\n",
        "        inject_y = np.random.binomial(1, self.missing_rate, size=values.shape)\n",
        "        inject_idx = np.where(inject_y.astype(np.bool))[0]\n",
        "        values = np.copy(values)\n",
        "        values[inject_idx] = -self.mean / self.std\n",
        "        missing = np.copy(missing)\n",
        "        missing[inject_idx] = 1\n",
        "        return values, labels, missing"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCixMjih2JHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training.py\n",
        "import six\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tfsnippet.scaffold import TrainLoop\n",
        "from tfsnippet.utils import (VarScopeObject,\n",
        "                             reopen_variable_scope,\n",
        "                             get_default_session_or_error,\n",
        "                             ensure_variables_initialized,\n",
        "                             get_variables_as_dict)\n",
        "\n",
        "###from .augmentation import MissingDataInjection\n",
        "###from .model import Donut\n",
        "###from .utils import BatchSlidingWindow\n",
        "\n",
        "###__all__ = ['DonutTrainer']\n",
        "\n",
        "\n",
        "class DonutTrainer(VarScopeObject):\n",
        "    \"\"\"\n",
        "    Donut trainer.\n",
        "    Args:\n",
        "        model (Donut): The :class:`Donut` model instance.\n",
        "        model_vs (str or tf.VariableScope): If specified, will collect\n",
        "            trainable variables only from this scope.  If :obj:`None`,\n",
        "            will collect all trainable variables within current graph.\n",
        "            (default :obj:`None`)\n",
        "        n_z (int or None): Number of `z` samples to take for each `x`.\n",
        "            (default :obj:`None`, one sample without explicit sampling\n",
        "            dimension)\n",
        "        feed_dict (dict[tf.Tensor, any]): User provided feed dict for\n",
        "            training. (default :obj:`None`, indicating no feeding)\n",
        "        valid_feed_dict (dict[tf.Tensor, any]): User provided feed dict for\n",
        "            validation.  If :obj:`None`, follow `feed_dict` of training.\n",
        "            (default :obj:`None`)\n",
        "        missing_data_injection_rate (float): Ratio of missing data injection.\n",
        "            (default 0.01)\n",
        "        use_regularization_loss (bool): Whether or not to add regularization\n",
        "            loss from `tf.GraphKeys.REGULARIZATION_LOSSES` to the training\n",
        "            loss? (default :obj:`True`)\n",
        "        max_epoch (int or None): Maximum epochs to run.  If :obj:`None`,\n",
        "            will not stop at any particular epoch. (default 256)\n",
        "        max_step (int or None): Maximum steps to run.  If :obj:`None`,\n",
        "            will not stop at any particular step.  At least one of `max_epoch`\n",
        "            and `max_step` should be specified. (default :obj:`None`)\n",
        "        batch_size (int): Size of mini-batches for training. (default 256)\n",
        "        valid_batch_size (int): Size of mini-batches for validation.\n",
        "            (default 1024)\n",
        "        valid_step_freq (int): Run validation after every `valid_step_freq`\n",
        "            number of training steps. (default 100)\n",
        "        initial_lr (float): Initial learning rate. (default 0.001)\n",
        "        lr_anneal_epochs (int): Anneal the learning rate after every\n",
        "            `lr_anneal_epochs` number of epochs. (default 10)\n",
        "        lr_anneal_factor (float): Anneal the learning rate with this\n",
        "            discount factor, i.e., ``learning_rate = learning_rate\n",
        "            * lr_anneal_factor``. (default 0.75)\n",
        "        optimizer (Type[tf.train.Optimizer]): The class of TensorFlow\n",
        "            optimizer. (default :class:`tf.train.AdamOptimizer`)\n",
        "        optimizer_params (dict[str, any] or None): The named arguments\n",
        "            for constructing the optimizer. (default :obj:`None`)\n",
        "        grad_clip_norm (float or None): Clip gradient by this norm.\n",
        "            If :obj:`None`, disable gradient clip by norm. (default 10.0)\n",
        "        check_numerics (bool): Whether or not to add TensorFlow assertions\n",
        "            for numerical issues? (default :obj:`True`)\n",
        "        name (str): Optional name of this trainer\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "        scope (str): Optional scope of this trainer\n",
        "            (argument of :class:`tfsnippet.utils.VarScopeObject`).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, model_vs=None, n_z=None,\n",
        "                 feed_dict=None, valid_feed_dict=None,\n",
        "                 missing_data_injection_rate=0.01,\n",
        "                 use_regularization_loss=True,\n",
        "                 max_epoch=256, max_step=None, batch_size=256,\n",
        "                 valid_batch_size=1024, valid_step_freq=100,\n",
        "                 initial_lr=0.001, lr_anneal_epochs=10, lr_anneal_factor=0.75,\n",
        "                 optimizer=tf.train.AdamOptimizer, optimizer_params=None,\n",
        "                 grad_clip_norm=10.0, check_numerics=True,\n",
        "                 name=None, scope=None):\n",
        "        super(DonutTrainer, self).__init__(name=name, scope=scope)\n",
        "\n",
        "        # memorize the arguments\n",
        "        self._model = model\n",
        "        self._n_z = n_z\n",
        "        if feed_dict is not None:\n",
        "            self._feed_dict = dict(six.iteritems(feed_dict))\n",
        "        else:\n",
        "            self._feed_dict = {}\n",
        "        if valid_feed_dict is not None:\n",
        "            self._valid_feed_dict = dict(six.iteritems(valid_feed_dict))\n",
        "        else:\n",
        "            self._valid_feed_dict = self._feed_dict\n",
        "        self._missing_data_injection_rate = missing_data_injection_rate\n",
        "        if max_epoch is None and max_step is None:\n",
        "            raise ValueError('At least one of `max_epoch` and `max_step` '\n",
        "                             'should be specified')\n",
        "        self._max_epoch = max_epoch\n",
        "        self._max_step = max_step\n",
        "        self._batch_size = batch_size\n",
        "        self._valid_batch_size = valid_batch_size\n",
        "        self._valid_step_freq = valid_step_freq\n",
        "        self._initial_lr = initial_lr\n",
        "        self._lr_anneal_epochs = lr_anneal_epochs\n",
        "        self._lr_anneal_factor = lr_anneal_factor\n",
        "\n",
        "        # build the trainer\n",
        "        with reopen_variable_scope(self.variable_scope):\n",
        "            # the global step for this model\n",
        "            self._global_step = tf.get_variable(\n",
        "                dtype=tf.int64, name='global_step', trainable=False,\n",
        "                initializer=tf.constant(0, dtype=tf.int64)\n",
        "            )\n",
        "\n",
        "            # input placeholders\n",
        "            self._input_x = tf.placeholder(\n",
        "                dtype=tf.float32, shape=[None, model.x_dims], name='input_x')\n",
        "            self._input_y = tf.placeholder(\n",
        "                dtype=tf.int32, shape=[None, model.x_dims], name='input_y')\n",
        "            self._learning_rate = tf.placeholder(\n",
        "                dtype=tf.float32, shape=(), name='learning_rate')\n",
        "\n",
        "            # compose the training loss\n",
        "            with tf.name_scope('loss'):\n",
        "                loss = model.get_training_loss(\n",
        "                    x=self._input_x, y=self._input_y, n_z=n_z)\n",
        "                if use_regularization_loss:\n",
        "                    loss += tf.losses.get_regularization_loss()\n",
        "                self._loss = loss\n",
        "\n",
        "            # get the training variables\n",
        "            train_params = get_variables_as_dict(\n",
        "                scope=model_vs, collection=tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "            self._train_params = train_params\n",
        "\n",
        "            # create the trainer\n",
        "            if optimizer_params is None:\n",
        "                optimizer_params = {}\n",
        "            else:\n",
        "                optimizer_params = dict(six.iteritems(optimizer_params))\n",
        "            optimizer_params['learning_rate'] = self._learning_rate\n",
        "            self._optimizer = optimizer(**optimizer_params)\n",
        "\n",
        "            # derive the training gradient\n",
        "            origin_grad_vars = self._optimizer.compute_gradients(\n",
        "                self._loss, list(six.itervalues(self._train_params))\n",
        "            )\n",
        "            grad_vars = []\n",
        "            for grad, var in origin_grad_vars:\n",
        "                if grad is not None and var is not None:\n",
        "                    if grad_clip_norm:\n",
        "                        grad = tf.clip_by_norm(grad, grad_clip_norm)\n",
        "                    if check_numerics:\n",
        "                        grad = tf.check_numerics(\n",
        "                            grad,\n",
        "                            'gradient for {} has numeric issue'.format(var.name)\n",
        "                        )\n",
        "                    grad_vars.append((grad, var))\n",
        "\n",
        "            # build the training op\n",
        "            with tf.control_dependencies(\n",
        "                    tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "                self._train_op = self._optimizer.apply_gradients(\n",
        "                    grad_vars, global_step=self._global_step)\n",
        "\n",
        "            # the training summary in case `summary_dir` is specified\n",
        "            with tf.name_scope('summary'):\n",
        "                self._summary_op = tf.summary.merge([\n",
        "                    tf.summary.histogram(v.name.rsplit(':', 1)[0], v)\n",
        "                    for v in six.itervalues(self._train_params)\n",
        "                ])\n",
        "\n",
        "            # initializer for the variables\n",
        "            self._trainer_initializer = tf.variables_initializer(\n",
        "                list(six.itervalues(self.get_variables_as_dict()))\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"\n",
        "        Get the :class:`Donut` model instance.\n",
        "        Returns:\n",
        "            Donut: The :class:`Donut` model instance.\n",
        "        \"\"\"\n",
        "        return self._model\n",
        "\n",
        "    def fit(self, values, labels, missing, mean, std, excludes=None,\n",
        "            valid_portion=0.3, summary_dir=None):\n",
        "        \"\"\"\n",
        "        Train the :class:`Donut` model with given data.\n",
        "        Args:\n",
        "            values (np.ndarray): 1-D `float32` array, the standardized\n",
        "                KPI observations.\n",
        "            labels (np.ndarray): 1-D `int32` array, the anomaly labels.\n",
        "            missing (np.ndarray): 1-D `int32` array, the indicator of\n",
        "                missing points.\n",
        "            mean (float): The mean of KPI observations before standardization.\n",
        "            std (float): The standard deviation of KPI observations before\n",
        "                standardization.\n",
        "            excludes (np.ndarray): 1-D `bool` array, indicators of whether\n",
        "                or not to totally exclude a point.  If a point is excluded,\n",
        "                any window which contains that point is excluded.\n",
        "                (default :obj:`None`, no point is totally excluded)\n",
        "            valid_portion (float): Ratio of validation data out of all the\n",
        "                specified training data. (default 0.3)\n",
        "            summary_dir (str): Optional summary directory for\n",
        "                :class:`tf.summary.FileWriter`. (default :obj:`None`,\n",
        "                summary is disabled)\n",
        "        \"\"\"\n",
        "        sess = get_default_session_or_error()\n",
        "\n",
        "        # split the training & validation set\n",
        "        values = np.asarray(values, dtype=np.float32)\n",
        "        labels = np.asarray(labels, dtype=np.int32)\n",
        "        missing = np.asarray(missing, dtype=np.int32)\n",
        "        if len(values.shape) != 1:\n",
        "            raise ValueError('`values` must be a 1-D array')\n",
        "        if labels.shape != values.shape:\n",
        "            raise ValueError('The shape of `labels` does not agree with '\n",
        "                             'the shape of `values` ({} vs {})'.\n",
        "                             format(labels.shape, values.shape))\n",
        "        if missing.shape != values.shape:\n",
        "            raise ValueError('The shape of `missing` does not agree with '\n",
        "                             'the shape of `values` ({} vs {})'.\n",
        "                             format(missing.shape, values.shape))\n",
        "\n",
        "        n = int(len(values) * valid_portion)\n",
        "        train_values, v_x = values[:-n], values[-n:]\n",
        "        train_labels, valid_labels = labels[:-n], labels[-n:]\n",
        "        train_missing, valid_missing = missing[:-n], missing[-n:]\n",
        "        v_y = np.logical_or(valid_labels, valid_missing).astype(np.int32)\n",
        "        if excludes is None:\n",
        "            train_excludes, valid_excludes = None, None\n",
        "        else:\n",
        "            train_excludes, valid_excludes = excludes[:-n], excludes[-n:]\n",
        "\n",
        "        # data augmentation object and the sliding window iterator\n",
        "        aug = MissingDataInjection(mean, std, self._missing_data_injection_rate)\n",
        "        train_sliding_window = BatchSlidingWindow(\n",
        "            array_size=len(train_values),\n",
        "            window_size=self.model.x_dims,\n",
        "            batch_size=self._batch_size,\n",
        "            excludes=train_excludes,\n",
        "            shuffle=True,\n",
        "            ignore_incomplete_batch=True,\n",
        "        )\n",
        "        valid_sliding_window = BatchSlidingWindow(\n",
        "            array_size=len(v_x),\n",
        "            window_size=self.model.x_dims,\n",
        "            batch_size=self._valid_batch_size,\n",
        "            excludes=valid_excludes,\n",
        "        )\n",
        "\n",
        "        # initialize the variables of the trainer, and the model\n",
        "        sess.run(self._trainer_initializer)\n",
        "        ensure_variables_initialized(self._train_params)\n",
        "\n",
        "        # training loop\n",
        "        lr = self._initial_lr\n",
        "        with TrainLoop(\n",
        "                param_vars=self._train_params,\n",
        "                early_stopping=True,\n",
        "                summary_dir=summary_dir,\n",
        "                max_epoch=self._max_epoch,\n",
        "                max_step=self._max_step) as loop:  # type: TrainLoop\n",
        "            loop.print_training_summary()\n",
        "\n",
        "            for epoch in loop.iter_epochs():\n",
        "                x, y1, y2 = aug.augment(\n",
        "                    train_values, train_labels, train_missing)\n",
        "                y = np.logical_or(y1, y2).astype(np.int32)\n",
        "\n",
        "                train_iterator = train_sliding_window.get_iterator([x, y])\n",
        "                for step, (batch_x, batch_y) in loop.iter_steps(train_iterator):\n",
        "                    # run a training step\n",
        "                    feed_dict = dict(six.iteritems(self._feed_dict))\n",
        "                    feed_dict[self._learning_rate] = lr\n",
        "                    feed_dict[self._input_x] = batch_x\n",
        "                    feed_dict[self._input_y] = batch_y\n",
        "                    loss, _ = sess.run(\n",
        "                        [self._loss, self._train_op], feed_dict=feed_dict)\n",
        "                    loop.collect_metrics({'loss': loss})\n",
        "\n",
        "                    if step % self._valid_step_freq == 0:\n",
        "                        # collect variable summaries\n",
        "                        if summary_dir is not None:\n",
        "                            loop.add_summary(sess.run(self._summary_op))\n",
        "\n",
        "                        # do validation in batches\n",
        "                        with loop.timeit('valid_time'), \\\n",
        "                                loop.metric_collector('valid_loss') as mc:\n",
        "                            v_it = valid_sliding_window.get_iterator([v_x, v_y])\n",
        "                            for b_v_x, b_v_y in v_it:\n",
        "                                feed_dict = dict(\n",
        "                                    six.iteritems(self._valid_feed_dict))\n",
        "                                feed_dict[self._input_x] = b_v_x\n",
        "                                feed_dict[self._input_y] = b_v_y\n",
        "                                loss = sess.run(self._loss, feed_dict=feed_dict)\n",
        "                                mc.collect(loss, weight=len(b_v_x))\n",
        "\n",
        "                        # print the logs of recent steps\n",
        "                        loop.print_logs()\n",
        "\n",
        "                # anneal the learning rate\n",
        "                if self._lr_anneal_epochs and \\\n",
        "                        epoch % self._lr_anneal_epochs == 0:\n",
        "                    lr *= self._lr_anneal_factor\n",
        "                    loop.println('Learning rate decreased to {}'.format(lr),\n",
        "                                 with_tag=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1F_550D2XUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utils.py\n",
        "import numpy as np\n",
        "\n",
        "###__all__ = ['minibatch_slices_iterator', 'BatchSlidingWindow']\n",
        "\n",
        "\n",
        "def minibatch_slices_iterator(length, batch_size,\n",
        "                              ignore_incomplete_batch=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the mini-batch slices.\n",
        "    Args:\n",
        "        length (int): Total length of data in an epoch.\n",
        "        batch_size (int): Size of each mini-batch.\n",
        "        ignore_incomplete_batch (bool): If :obj:`True`, discard the final\n",
        "            batch if it contains less than `batch_size` number of items.\n",
        "            (default :obj:`False`)\n",
        "    Yields\n",
        "        slice: Slices of each mini-batch.  The last mini-batch may contain\n",
        "               less indices than `batch_size`.\n",
        "    \"\"\"\n",
        "    start = 0\n",
        "    stop1 = (length // batch_size) * batch_size\n",
        "    while start < stop1:\n",
        "        yield slice(start, start + batch_size, 1)\n",
        "        start += batch_size\n",
        "    if not ignore_incomplete_batch and start < length:\n",
        "        yield slice(start, length, 1)\n",
        "\n",
        "\n",
        "class BatchSlidingWindow(object):\n",
        "    \"\"\"\n",
        "    Class for obtaining mini-batch iterators of sliding windows.\n",
        "    Each mini-batch will have `batch_size` windows.  If the final batch\n",
        "    contains less than `batch_size` windows, it will be discarded if\n",
        "    `ignore_incomplete_batch` is :obj:`True`.\n",
        "    Args:\n",
        "        array_size (int): Size of the arrays to be iterated.\n",
        "        window_size (int): The size of the windows.\n",
        "        batch_size (int): Size of each mini-batch.\n",
        "        excludes (np.ndarray): 1-D `bool` array, indicators of whether\n",
        "            or not to totally exclude a point.  If a point is excluded,\n",
        "            any window which contains that point is excluded.\n",
        "            (default :obj:`None`, no point is totally excluded)\n",
        "        shuffle (bool): If :obj:`True`, the windows will be iterated in\n",
        "            shuffled order. (default :obj:`False`)\n",
        "        ignore_incomplete_batch (bool): If :obj:`True`, discard the final\n",
        "            batch if it contains less than `batch_size` number of windows.\n",
        "            (default :obj:`False`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, array_size, window_size, batch_size, excludes=None,\n",
        "                 shuffle=False, ignore_incomplete_batch=False):\n",
        "        # check the parameters\n",
        "        if window_size < 1:\n",
        "            raise ValueError('`window_size` must be at least 1')\n",
        "        if array_size < window_size:\n",
        "            raise ValueError('`array_size` must be at least as large as '\n",
        "                             '`window_size`')\n",
        "        if excludes is not None:\n",
        "            excludes = np.asarray(excludes, dtype=np.bool)\n",
        "            expected_shape = (array_size,)\n",
        "            if excludes.shape != expected_shape:\n",
        "                raise ValueError('The shape of `excludes` is expected to be '\n",
        "                                 '{}, but got {}'.\n",
        "                                 format(expected_shape, excludes.shape))\n",
        "\n",
        "        # compute which points are not excluded\n",
        "        if excludes is not None:\n",
        "            mask = np.logical_not(excludes)\n",
        "        else:\n",
        "            mask = np.ones([array_size], dtype=np.bool)\n",
        "        mask[: window_size - 1] = False\n",
        "        where_excludes = np.where(excludes)[0]\n",
        "        for k in range(1, window_size):\n",
        "            also_excludes = where_excludes + k\n",
        "            also_excludes = also_excludes[also_excludes < array_size]\n",
        "            mask[also_excludes] = False\n",
        "\n",
        "        # generate the indices of window endings\n",
        "        indices = np.arange(array_size)[mask]\n",
        "        self._indices = indices.reshape([-1, 1])\n",
        "\n",
        "        # the offset array to generate the windows\n",
        "        self._offsets = np.arange(-window_size + 1, 1)\n",
        "\n",
        "        # memorize arguments\n",
        "        self._array_size = array_size\n",
        "        self._window_size = window_size\n",
        "        self._batch_size = batch_size\n",
        "        self._shuffle = shuffle\n",
        "        self._ignore_incomplete_batch = ignore_incomplete_batch\n",
        "\n",
        "    def get_iterator(self, arrays):\n",
        "        \"\"\"\n",
        "        Iterate through the sliding windows of each array in `arrays`.\n",
        "        This method is not re-entrant, i.e., calling :meth:`get_iterator`\n",
        "        would invalidate any previous obtained iterator.\n",
        "        Args:\n",
        "            arrays (Iterable[np.ndarray]): 1-D arrays to be iterated.\n",
        "        Yields:\n",
        "            tuple[np.ndarray]: The windows of arrays of each mini-batch.\n",
        "        \"\"\"\n",
        "        # check the parameters\n",
        "        arrays = tuple(np.asarray(a) for a in arrays)\n",
        "        if not arrays:\n",
        "            raise ValueError('`arrays` must not be empty')\n",
        "        expected_shape = (self._array_size,)\n",
        "        for i, a in enumerate(arrays):\n",
        "            if a.shape != expected_shape:\n",
        "                raise ValueError('The shape of `arrays[{}]` is expected to '\n",
        "                                 'be {}, but got {}'.\n",
        "                                 format(i, expected_shape, a.shape))\n",
        "\n",
        "        # shuffle if required\n",
        "        if self._shuffle:\n",
        "            np.random.shuffle(self._indices)\n",
        "\n",
        "        # iterate through the mini-batches\n",
        "        for s in minibatch_slices_iterator(\n",
        "                length=len(self._indices),\n",
        "                batch_size=self._batch_size,\n",
        "                ignore_incomplete_batch=self._ignore_incomplete_batch):\n",
        "            idx = self._indices[s] + self._offsets\n",
        "            yield tuple(a[idx] for a in arrays)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzEPUaaIEcT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare the data\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "###import complete_timestamp, standardize_kpi\n",
        "\n",
        "# Read the raw data.\n",
        "dFrame = read_csv('cpu4.csv', header=0, squeeze=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u44NwLaV6--7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp, values, labels = dFrame['timestamp'], dFrame['value'], dFrame['label']\n",
        "\n",
        "# If there is no label, simply use all zeros.\n",
        "labels = np.zeros_like(values, dtype=np.int32)\n",
        "\n",
        "# Complete the timestamp, and obtain the missing point indicators.\n",
        "timestamp, missing, (values, labels) = \\\n",
        "    complete_timestamp(timestamp, (values, labels))\n",
        "\n",
        "# Split the training and testing data.\n",
        "test_portion = 0.3\n",
        "test_n = int(len(values) * test_portion)\n",
        "train_values, test_values = values[:-test_n], values[-test_n:]\n",
        "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
        "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
        "\n",
        "# Standardize the training and testing data.\n",
        "train_values, mean, std = standardize_kpi(\n",
        "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
        "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az2YXSAWFAO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "###from donut import Donut\n",
        "from tensorflow import keras as K\n",
        "from tfsnippet.modules import Sequential\n",
        "\n",
        "# We build the entire model within the scope of `model_vs`,\n",
        "# it should hold exactly all the variables of `model`, including\n",
        "# the variables created by Keras layers.\n",
        "with tf.variable_scope('model') as model_vs:\n",
        "    model = Donut(\n",
        "        h_for_p_x=Sequential([\n",
        "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
        "                           activation=tf.nn.relu),\n",
        "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
        "                           activation=tf.nn.relu),\n",
        "        ]),\n",
        "        h_for_q_z=Sequential([\n",
        "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
        "                           activation=tf.nn.relu),\n",
        "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
        "                           activation=tf.nn.relu),\n",
        "        ]),\n",
        "        x_dims=120,\n",
        "        z_dims=5,\n",
        "    )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhzMaJRaFJyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bd9c66a-38a2-4f99-fb7e-438e366395b2"
      },
      "source": [
        "###from donut import DonutTrainer, DonutPredictor\n",
        "\n",
        "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
        "predictor = DonutPredictor(model)\n",
        "\n",
        "with tf.Session().as_default():\n",
        "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
        "    test_score = predictor.get_score(test_values, test_missing)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Trainable Parameters              (58,150 in total)\n",
            "---------------------------------------------------\n",
            "donut/p_x_given_z/x_mean/bias    (120,)         120\n",
            "donut/p_x_given_z/x_mean/kernel  (100, 120)  12,000\n",
            "donut/p_x_given_z/x_std/bias     (120,)         120\n",
            "donut/p_x_given_z/x_std/kernel   (100, 120)  12,000\n",
            "donut/q_z_given_x/z_mean/bias    (5,)             5\n",
            "donut/q_z_given_x/z_mean/kernel  (100, 5)       500\n",
            "donut/q_z_given_x/z_std/bias     (5,)             5\n",
            "donut/q_z_given_x/z_std/kernel   (100, 5)       500\n",
            "sequential/_0/dense/bias         (100,)         100\n",
            "sequential/_0/dense/kernel       (5, 100)       500\n",
            "sequential/_1/dense/bias         (100,)         100\n",
            "sequential/_1/dense/kernel       (100, 100)  10,000\n",
            "sequential_1/_0/dense/bias       (100,)         100\n",
            "sequential_1/_0/dense/kernel     (120, 100)  12,000\n",
            "sequential_1/_1/dense/bias       (100,)         100\n",
            "sequential_1/_1/dense/kernel     (100, 100)  10,000\n",
            "\n",
            "[Epoch 4/256, Step 100, ETA 2m 4.53s] step time: 0.01419s (±0.01906s); valid time: 0.1551s; loss: 85.2972 (±59.3297); valid loss: 30.5699 (*)\n",
            "[Epoch 7/256, Step 200, ETA 1m 56.11s] step time: 0.01262s (±0.01269s); valid time: 0.1268s; loss: -11.422 (±6.23579); valid loss: 17.7889 (*)\n",
            "[Epoch 10/256, Step 300, ETA 1m 50.22s] step time: 0.01181s (±0.005147s); valid time: 0.05073s; loss: -27.7251 (±5.18409); valid loss: 17.7905\n",
            "[Epoch 10/256, Step 330, ETA 1m 48.41s] Learning rate decreased to 0.00075\n",
            "[Epoch 13/256, Step 400, ETA 1m 48.42s] step time: 0.01268s (±0.01324s); valid time: 0.1327s; loss: -39.9612 (±3.7386); valid loss: 15.2912 (*)\n",
            "[Epoch 16/256, Step 500, ETA 1m 45.25s] step time: 0.01173s (±0.005022s); valid time: 0.05016s; loss: -46.7828 (±2.72966); valid loss: 16.8038\n",
            "[Epoch 19/256, Step 600, ETA 1m 44.37s] step time: 0.01293s (±0.01275s); valid time: 0.1274s; loss: -51.2519 (±2.81837); valid loss: 14.3474 (*)\n",
            "[Epoch 20/256, Step 660, ETA 1m 42.64s] Learning rate decreased to 0.0005625000000000001\n",
            "[Epoch 22/256, Step 700, ETA 1m 42.23s] step time: 0.01194s (±0.00518s); valid time: 0.0509s; loss: -53.7603 (±2.56836); valid loss: 18.3523\n",
            "[Epoch 25/256, Step 800, ETA 1m 40.18s] step time: 0.01182s (±0.005499s); valid time: 0.05043s; loss: -56.191 (±2.51996); valid loss: 17.4511\n",
            "[Epoch 28/256, Step 900, ETA 1m 38.27s] step time: 0.01178s (±0.005035s); valid time: 0.04995s; loss: -58.1079 (±2.1838); valid loss: 18.1621\n",
            "[Epoch 30/256, Step 990, ETA 1m 36.2s] Learning rate decreased to 0.00042187500000000005\n",
            "[Epoch 31/256, Step 1000, ETA 1m 36.4s] step time: 0.01166s (±0.004995s); valid time: 0.04946s; loss: -59.9871 (±2.05735); valid loss: 16.1344\n",
            "[Epoch 34/256, Step 1100, ETA 1m 34.68s] step time: 0.0117s (±0.00504s); valid time: 0.05055s; loss: -60.5123 (±2.39596); valid loss: 14.731\n",
            "[Epoch 37/256, Step 1200, ETA 1m 33.11s] step time: 0.01182s (±0.005077s); valid time: 0.05017s; loss: -61.9206 (±2.3925); valid loss: 17.3822\n",
            "[Epoch 40/256, Step 1300, ETA 1m 31.6s] step time: 0.01184s (±0.005203s); valid time: 0.05111s; loss: -62.6637 (±1.92461); valid loss: 17.308\n",
            "[Epoch 40/256, Step 1320, ETA 1m 31.25s] Learning rate decreased to 0.00031640625000000006\n",
            "[Epoch 43/256, Step 1400, ETA 1m 30.36s] step time: 0.01227s (±0.005571s); valid time: 0.05448s; loss: -63.5665 (±2.37478); valid loss: 18.8776\n",
            "[Epoch 46/256, Step 1500, ETA 1m 29.09s] step time: 0.01225s (±0.005279s); valid time: 0.05263s; loss: -64.5228 (±2.19734); valid loss: 18.7205\n",
            "[Epoch 49/256, Step 1600, ETA 1m 27.84s] step time: 0.01228s (±0.005488s); valid time: 0.05369s; loss: -64.9047 (±2.28414); valid loss: 17.1431\n",
            "[Epoch 50/256, Step 1650, ETA 1m 27.08s] Learning rate decreased to 0.00023730468750000005\n",
            "[Epoch 52/256, Step 1700, ETA 1m 26.68s] step time: 0.0125s (±0.005525s); valid time: 0.05437s; loss: -65.9174 (±1.80563); valid loss: 19.2117\n",
            "[Epoch 55/256, Step 1800, ETA 1m 25.43s] step time: 0.01233s (±0.005435s); valid time: 0.05379s; loss: -66.3154 (±2.27452); valid loss: 19.326\n",
            "[Epoch 58/256, Step 1900, ETA 1m 24.13s] step time: 0.01218s (±0.005437s); valid time: 0.05436s; loss: -66.6697 (±2.37429); valid loss: 22.2244\n",
            "[Epoch 60/256, Step 1980, ETA 1m 22.96s] Learning rate decreased to 0.00017797851562500002\n",
            "[Epoch 61/256, Step 2000, ETA 1m 22.89s] step time: 0.01238s (±0.006447s); valid time: 0.06391s; loss: -67.0628 (±2.15856); valid loss: 23.3302\n",
            "[Epoch 64/256, Step 2100, ETA 1m 21.59s] step time: 0.01215s (±0.005313s); valid time: 0.05298s; loss: -67.3409 (±2.36627); valid loss: 23.5984\n",
            "[Epoch 67/256, Step 2200, ETA 1m 20.35s] step time: 0.0124s (±0.005336s); valid time: 0.05222s; loss: -67.653 (±2.02068); valid loss: 24.0394\n",
            "[Epoch 70/256, Step 2300, ETA 1m 18.97s] step time: 0.01187s (±0.005336s); valid time: 0.05271s; loss: -67.8737 (±2.23255); valid loss: 21.3805\n",
            "[Epoch 70/256, Step 2310, ETA 1m 18.8s] Learning rate decreased to 0.00013348388671875002\n",
            "[Epoch 73/256, Step 2400, ETA 1m 17.6s] step time: 0.01192s (±0.006019s); valid time: 0.05579s; loss: -68.7531 (±2.14238); valid loss: 25.8669\n",
            "[Epoch 76/256, Step 2500, ETA 1m 16.22s] step time: 0.01181s (±0.005093s); valid time: 0.05102s; loss: -68.497 (±1.87899); valid loss: 24.2763\n",
            "[Epoch 79/256, Step 2600, ETA 1m 14.87s] step time: 0.01192s (±0.005261s); valid time: 0.05159s; loss: -69.1276 (±1.83348); valid loss: 24.3858\n",
            "[Epoch 80/256, Step 2640, ETA 1m 14.31s] Learning rate decreased to 0.00010011291503906251\n",
            "[Epoch 82/256, Step 2700, ETA 1m 13.55s] step time: 0.01191s (±0.005105s); valid time: 0.05045s; loss: -69.3964 (±1.92956); valid loss: 25.7013\n",
            "[Epoch 85/256, Step 2800, ETA 1m 12.15s] step time: 0.01163s (±0.005114s); valid time: 0.0507s; loss: -69.5075 (±1.93466); valid loss: 27.0582\n",
            "[Epoch 88/256, Step 2900, ETA 1m 10.79s] step time: 0.01173s (±0.005159s); valid time: 0.05134s; loss: -69.5929 (±2.11784); valid loss: 26.4946\n",
            "[Epoch 90/256, Step 2970, ETA 1m 9.848s] Learning rate decreased to 7.508468627929689e-05\n",
            "[Epoch 91/256, Step 3000, ETA 1m 9.517s] step time: 0.01216s (±0.00513s); valid time: 0.05098s; loss: -69.9774 (±2.03138); valid loss: 27.0347\n",
            "[Epoch 94/256, Step 3100, ETA 1m 8.218s] step time: 0.01202s (±0.005279s); valid time: 0.0521s; loss: -70.0025 (±2.19128); valid loss: 26.1881\n",
            "[Epoch 97/256, Step 3200, ETA 1m 6.882s] step time: 0.01179s (±0.005076s); valid time: 0.05055s; loss: -69.7472 (±1.95589); valid loss: 26.2685\n",
            "[Epoch 100/256, Step 3300, ETA 1m 5.572s] step time: 0.01191s (±0.005123s); valid time: 0.05046s; loss: -69.9725 (±2.04595); valid loss: 25.8504\n",
            "[Epoch 100/256, Step 3300, ETA 1m 5.573s] Learning rate decreased to 5.631351470947266e-05\n",
            "[Epoch 104/256, Step 3400, ETA 1m 4.261s] step time: 0.01177s (±0.005033s); valid time: 0.04909s; loss: -70.0867 (±2.05778); valid loss: 25.7816\n",
            "[Epoch 107/256, Step 3500, ETA 1m 2.918s] step time: 0.01163s (±0.005031s); valid time: 0.05028s; loss: -70.6085 (±2.27077); valid loss: 26.8821\n",
            "[Epoch 110/256, Step 3600, ETA 1m 1.608s] step time: 0.01181s (±0.005133s); valid time: 0.05043s; loss: -70.3028 (±2.2097); valid loss: 25.8611\n",
            "[Epoch 110/256, Step 3630, ETA 1m 1.192s] Learning rate decreased to 4.22351360321045e-05\n",
            "[Epoch 113/256, Step 3700, ETA 1m 0.2929s] step time: 0.01174s (±0.005167s); valid time: 0.05095s; loss: -70.4421 (±2.00216); valid loss: 26.3831\n",
            "[Epoch 116/256, Step 3800, ETA 58.99s] step time: 0.0118s (±0.004988s); valid time: 0.04954s; loss: -70.5307 (±1.85193); valid loss: 26.5777\n",
            "[Epoch 119/256, Step 3900, ETA 57.7s] step time: 0.01194s (±0.005163s); valid time: 0.05087s; loss: -70.3403 (±1.75644); valid loss: 25.7084\n",
            "[Epoch 120/256, Step 3960, ETA 56.88s] Learning rate decreased to 3.167635202407837e-05\n",
            "[Epoch 122/256, Step 4000, ETA 56.4s] step time: 0.0118s (±0.005247s); valid time: 0.05179s; loss: -70.4841 (±2.02392); valid loss: 26.3267\n",
            "[Epoch 125/256, Step 4100, ETA 55.1s] step time: 0.0118s (±0.005078s); valid time: 0.05077s; loss: -70.6011 (±2.04899); valid loss: 26.023\n",
            "[Epoch 128/256, Step 4200, ETA 53.82s] step time: 0.01191s (±0.005168s); valid time: 0.05076s; loss: -70.7122 (±1.85862); valid loss: 26.2069\n",
            "[Epoch 130/256, Step 4290, ETA 52.6s] Learning rate decreased to 2.3757264018058778e-05\n",
            "[Epoch 131/256, Step 4300, ETA 52.52s] step time: 0.01177s (±0.005096s); valid time: 0.0508s; loss: -70.6981 (±2.11244); valid loss: 27.3727\n",
            "[Epoch 134/256, Step 4400, ETA 51.24s] step time: 0.01191s (±0.005202s); valid time: 0.0513s; loss: -70.9876 (±1.85815); valid loss: 27.4018\n",
            "[Epoch 137/256, Step 4500, ETA 49.94s] step time: 0.01167s (±0.005171s); valid time: 0.05157s; loss: -70.5633 (±2.06028); valid loss: 26.568\n",
            "[Epoch 140/256, Step 4600, ETA 48.65s] step time: 0.01166s (±0.005112s); valid time: 0.05073s; loss: -70.7413 (±1.97825); valid loss: 26.6017\n",
            "[Epoch 140/256, Step 4620, ETA 48.38s] Learning rate decreased to 1.7817948013544083e-05\n",
            "[Epoch 143/256, Step 4700, ETA 47.36s] step time: 0.01175s (±0.00521s); valid time: 0.05134s; loss: -71.0043 (±1.90111); valid loss: 26.6238\n",
            "[Epoch 146/256, Step 4800, ETA 46.09s] step time: 0.01201s (±0.005247s); valid time: 0.05136s; loss: -70.8736 (±1.9632); valid loss: 26.8696\n",
            "[Epoch 149/256, Step 4900, ETA 44.81s] step time: 0.01177s (±0.005155s); valid time: 0.05097s; loss: -70.8382 (±2.143); valid loss: 26.1964\n",
            "[Epoch 150/256, Step 4950, ETA 44.15s] Learning rate decreased to 1.3363461010158061e-05\n",
            "[Epoch 152/256, Step 5000, ETA 43.54s] step time: 0.0118s (±0.005103s); valid time: 0.05079s; loss: -71.048 (±1.79486); valid loss: 27.2652\n",
            "[Epoch 155/256, Step 5100, ETA 42.26s] step time: 0.01183s (±0.005192s); valid time: 0.0512s; loss: -71.2597 (±2.07242); valid loss: 26.9914\n",
            "[Epoch 158/256, Step 5200, ETA 40.99s] step time: 0.01178s (±0.005048s); valid time: 0.05023s; loss: -70.7871 (±2.11137); valid loss: 25.9477\n",
            "[Epoch 160/256, Step 5280, ETA 39.94s] Learning rate decreased to 1.0022595757618546e-05\n",
            "[Epoch 161/256, Step 5300, ETA 39.71s] step time: 0.01176s (±0.005228s); valid time: 0.05195s; loss: -71.1668 (±1.95367); valid loss: 26.3325\n",
            "[Epoch 164/256, Step 5400, ETA 38.44s] step time: 0.01193s (±0.005057s); valid time: 0.05003s; loss: -71.4287 (±2.17119); valid loss: 27.0127\n",
            "[Epoch 167/256, Step 5500, ETA 37.17s] step time: 0.01179s (±0.006148s); valid time: 0.06061s; loss: -70.5479 (±1.94621); valid loss: 26.2998\n",
            "[Epoch 170/256, Step 5600, ETA 35.91s] step time: 0.01194s (±0.005245s); valid time: 0.05177s; loss: -71.1832 (±1.68209); valid loss: 27.1194\n",
            "[Epoch 170/256, Step 5610, ETA 35.78s] Learning rate decreased to 7.51694681821391e-06\n",
            "[Epoch 173/256, Step 5700, ETA 34.63s] step time: 0.01167s (±0.005078s); valid time: 0.05065s; loss: -71.1444 (±2.0653); valid loss: 26.9232\n",
            "[Epoch 176/256, Step 5800, ETA 33.36s] step time: 0.0117s (±0.005176s); valid time: 0.05175s; loss: -71.083 (±2.04067); valid loss: 27.2153\n",
            "[Epoch 179/256, Step 5900, ETA 32.08s] step time: 0.01167s (±0.005148s); valid time: 0.0511s; loss: -71.1233 (±1.96309); valid loss: 26.8577\n",
            "[Epoch 180/256, Step 5940, ETA 31.56s] Learning rate decreased to 5.637710113660432e-06\n",
            "[Epoch 182/256, Step 6000, ETA 30.82s] step time: 0.01185s (±0.005182s); valid time: 0.05146s; loss: -71.5867 (±2.05145); valid loss: 27.4313\n",
            "[Epoch 185/256, Step 6100, ETA 29.55s] step time: 0.01167s (±0.005013s); valid time: 0.05004s; loss: -71.4234 (±1.87297); valid loss: 26.9658\n",
            "[Epoch 188/256, Step 6200, ETA 28.28s] step time: 0.01171s (±0.0051s); valid time: 0.05095s; loss: -71.2222 (±1.72406); valid loss: 27.2419\n",
            "[Epoch 190/256, Step 6270, ETA 27.38s] Learning rate decreased to 4.228282585245324e-06\n",
            "[Epoch 191/256, Step 6300, ETA 27.01s] step time: 0.01166s (±0.005005s); valid time: 0.04964s; loss: -71.2598 (±2.00386); valid loss: 27.2623\n",
            "[Epoch 194/256, Step 6400, ETA 25.74s] step time: 0.0117s (±0.004924s); valid time: 0.04893s; loss: -71.5091 (±1.93179); valid loss: 27.2345\n",
            "[Epoch 197/256, Step 6500, ETA 24.49s] step time: 0.012s (±0.005257s); valid time: 0.05112s; loss: -71.4278 (±1.97019); valid loss: 27.4049\n",
            "[Epoch 200/256, Step 6600, ETA 23.23s] step time: 0.01184s (±0.00521s); valid time: 0.05168s; loss: -71.4932 (±1.96742); valid loss: 27.3468\n",
            "[Epoch 200/256, Step 6600, ETA 23.23s] Learning rate decreased to 3.171211938933993e-06\n",
            "[Epoch 204/256, Step 6700, ETA 21.97s] step time: 0.01183s (±0.005152s); valid time: 0.05058s; loss: -71.4319 (±1.95317); valid loss: 27.2598\n",
            "[Epoch 207/256, Step 6800, ETA 20.71s] step time: 0.01171s (±0.005089s); valid time: 0.05016s; loss: -71.6215 (±2.23147); valid loss: 27.223\n",
            "[Epoch 210/256, Step 6900, ETA 19.45s] step time: 0.01176s (±0.004973s); valid time: 0.04935s; loss: -71.1824 (±1.99233); valid loss: 27.3504\n",
            "[Epoch 210/256, Step 6930, ETA 19.06s] Learning rate decreased to 2.3784089542004944e-06\n",
            "[Epoch 213/256, Step 7000, ETA 18.18s] step time: 0.0117s (±0.005159s); valid time: 0.05104s; loss: -71.4077 (±1.81749); valid loss: 26.8319\n",
            "[Epoch 216/256, Step 7100, ETA 16.92s] step time: 0.01164s (±0.00507s); valid time: 0.05052s; loss: -71.4478 (±1.78803); valid loss: 27.817\n",
            "[Epoch 219/256, Step 7200, ETA 15.66s] step time: 0.01169s (±0.005088s); valid time: 0.05055s; loss: -71.3307 (±2.03697); valid loss: 27.1162\n",
            "[Epoch 220/256, Step 7260, ETA 14.9s] Learning rate decreased to 1.7838067156503708e-06\n",
            "[Epoch 222/256, Step 7300, ETA 14.41s] step time: 0.01178s (±0.005008s); valid time: 0.05008s; loss: -71.3291 (±1.94491); valid loss: 27.1144\n",
            "[Epoch 225/256, Step 7400, ETA 13.15s] step time: 0.01213s (±0.005164s); valid time: 0.05096s; loss: -71.5041 (±2.15285); valid loss: 27.3819\n",
            "[Epoch 228/256, Step 7500, ETA 11.9s] step time: 0.01187s (±0.005464s); valid time: 0.05135s; loss: -71.3881 (±1.9459); valid loss: 26.956\n",
            "[Epoch 230/256, Step 7590, ETA 10.76s] Learning rate decreased to 1.337855036737778e-06\n",
            "[Epoch 231/256, Step 7600, ETA 10.64s] step time: 0.01174s (±0.005086s); valid time: 0.05004s; loss: -71.0657 (±1.88758); valid loss: 27.2981\n",
            "[Epoch 234/256, Step 7700, ETA 9.382s] step time: 0.01164s (±0.004969s); valid time: 0.04948s; loss: -71.4518 (±2.0775); valid loss: 27.4155\n",
            "[Epoch 237/256, Step 7800, ETA 8.127s] step time: 0.01186s (±0.005881s); valid time: 0.05118s; loss: -71.2438 (±1.72358); valid loss: 26.8911\n",
            "[Epoch 240/256, Step 7900, ETA 6.872s] step time: 0.01177s (±0.0051s); valid time: 0.05069s; loss: -71.4068 (±1.89127); valid loss: 27.4104\n",
            "[Epoch 240/256, Step 7920, ETA 6.62s] Learning rate decreased to 1.0033912775533336e-06\n",
            "[Epoch 243/256, Step 8000, ETA 5.618s] step time: 0.01186s (±0.005121s); valid time: 0.05088s; loss: -71.0989 (±2.17763); valid loss: 26.8271\n",
            "[Epoch 246/256, Step 8100, ETA 4.363s] step time: 0.01169s (±0.005053s); valid time: 0.0506s; loss: -71.3727 (±1.92579); valid loss: 27.0376\n",
            "[Epoch 249/256, Step 8200, ETA 3.108s] step time: 0.01171s (±0.004981s); valid time: 0.04944s; loss: -71.4331 (±1.89133); valid loss: 27.1922\n",
            "[Epoch 250/256, Step 8250, ETA 2.481s] Learning rate decreased to 7.525434581650002e-07\n",
            "[Epoch 252/256, Step 8300, ETA 1.855s] step time: 0.01178s (±0.005146s); valid time: 0.04947s; loss: -71.1957 (±1.96885); valid loss: 26.7622\n",
            "[Epoch 255/256, Step 8400, ETA 0.6016s] step time: 0.01189s (±0.005235s); valid time: 0.05125s; loss: -71.3865 (±2.06393); valid loss: 27.2077\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpx2nr2t89/variables.dat-600\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}