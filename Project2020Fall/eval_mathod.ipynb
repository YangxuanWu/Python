{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval_mathod.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOI1DZqexjC1G3EkPVXbp1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangxuanWu/Python/blob/master/Project2020Fall/eval_mathod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdy2PFXWxhs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "\n",
        "from omni_anomaly.spot import SPOT\n",
        "\n",
        "\n",
        "def calc_point2point(predict, actual):\n",
        "    \"\"\"\n",
        "    calculate f1 score by predict and actual.\n",
        "\n",
        "    Args:\n",
        "        predict (np.ndarray): the predict label\n",
        "        actual (np.ndarray): np.ndarray\n",
        "    \"\"\"\n",
        "    TP = np.sum(predict * actual)\n",
        "    TN = np.sum((1 - predict) * (1 - actual))\n",
        "    FP = np.sum(predict * (1 - actual))\n",
        "    FN = np.sum((1 - predict) * actual)\n",
        "    precision = TP / (TP + FP + 0.00001)\n",
        "    recall = TP / (TP + FN + 0.00001)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 0.00001)\n",
        "    return f1, precision, recall, TP, TN, FP, FN\n",
        "\n",
        "\n",
        "def adjust_predicts(score, label,\n",
        "                    threshold=None,\n",
        "                    pred=None,\n",
        "                    calc_latency=False):\n",
        "    \"\"\"\n",
        "    Calculate adjusted predict labels using given `score`, `threshold` (or given `pred`) and `label`.\n",
        "\n",
        "    Args:\n",
        "        score (np.ndarray): The anomaly score\n",
        "        label (np.ndarray): The ground-truth label\n",
        "        threshold (float): The threshold of anomaly score.\n",
        "            A point is labeled as \"anomaly\" if its score is lower than the threshold.\n",
        "        pred (np.ndarray or None): if not None, adjust `pred` and ignore `score` and `threshold`,\n",
        "        calc_latency (bool):\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: predict labels\n",
        "    \"\"\"\n",
        "    if len(score) != len(label):\n",
        "        raise ValueError(\"score and label must have the same length\")\n",
        "    score = np.asarray(score)\n",
        "    label = np.asarray(label)\n",
        "    latency = 0\n",
        "    if pred is None:\n",
        "        predict = score < threshold\n",
        "    else:\n",
        "        predict = pred\n",
        "    actual = label > 0.1\n",
        "    anomaly_state = False\n",
        "    anomaly_count = 0\n",
        "    for i in range(len(score)):\n",
        "        if actual[i] and predict[i] and not anomaly_state:\n",
        "                anomaly_state = True\n",
        "                anomaly_count += 1\n",
        "                for j in range(i, 0, -1):\n",
        "                    if not actual[j]:\n",
        "                        break\n",
        "                    else:\n",
        "                        if not predict[j]:\n",
        "                            predict[j] = True\n",
        "                            latency += 1\n",
        "        elif not actual[i]:\n",
        "            anomaly_state = False\n",
        "        if anomaly_state:\n",
        "            predict[i] = True\n",
        "    if calc_latency:\n",
        "        return predict, latency / (anomaly_count + 1e-4)\n",
        "    else:\n",
        "        return predict\n",
        "\n",
        "\n",
        "def calc_seq(score, label, threshold, calc_latency=False):\n",
        "    \"\"\"\n",
        "    Calculate f1 score for a score sequence\n",
        "    \"\"\"\n",
        "    if calc_latency:\n",
        "        predict, latency = adjust_predicts(score, label, threshold, calc_latency=calc_latency)\n",
        "        t = list(calc_point2point(predict, label))\n",
        "        t.append(latency)\n",
        "        return t\n",
        "    else:\n",
        "        predict = adjust_predicts(score, label, threshold, calc_latency=calc_latency)\n",
        "        return calc_point2point(predict, label)\n",
        "\n",
        "\n",
        "def bf_search(score, label, start, end=None, step_num=1, display_freq=1, verbose=True):\n",
        "    \"\"\"\n",
        "    Find the best-f1 score by searching best `threshold` in [`start`, `end`).\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        list: list for results\n",
        "        float: the `threshold` for best-f1\n",
        "    \"\"\"\n",
        "    if step_num is None or end is None:\n",
        "        end = start\n",
        "        step_num = 1\n",
        "    search_step, search_range, search_lower_bound = step_num, end - start, start\n",
        "    if verbose:\n",
        "        print(\"search range: \", search_lower_bound, search_lower_bound + search_range)\n",
        "    threshold = search_lower_bound\n",
        "    m = (-1., -1., -1.)\n",
        "    m_t = 0.0\n",
        "    for i in range(search_step):\n",
        "        threshold += search_range / float(search_step)\n",
        "        target = calc_seq(score, label, threshold, calc_latency=True)\n",
        "        if target[0] > m[0]:\n",
        "            m_t = threshold\n",
        "            m = target\n",
        "        if verbose and i % display_freq == 0:\n",
        "            print(\"cur thr: \", threshold, target, m, m_t)\n",
        "    print(m, m_t)\n",
        "    return m, m_t\n",
        "\n",
        "\n",
        "def pot_eval(init_score, score, label, q=1e-3, level=0.02):\n",
        "    \"\"\"\n",
        "    Run POT method on given score.\n",
        "    Args:\n",
        "        init_score (np.ndarray): The data to get init threshold.\n",
        "            For `OmniAnomaly`, it should be the anomaly score of train set.\n",
        "        score (np.ndarray): The data to run POT method.\n",
        "            For `OmniAnomaly`, it should be the anomaly score of test set.\n",
        "        label:\n",
        "        q (float): Detection level (risk)\n",
        "        level (float): Probability associated with the initial threshold t\n",
        "\n",
        "    Returns:\n",
        "        dict: pot result dict\n",
        "    \"\"\"\n",
        "    s = SPOT(q)  # SPOT object\n",
        "    s.fit(init_score, score)  # data import\n",
        "    s.initialize(level=level, min_extrema=True)  # initialization step\n",
        "    ret = s.run(dynamic=False)  # run\n",
        "    print(len(ret['alarms']))\n",
        "    print(len(ret['thresholds']))\n",
        "    pot_th = -np.mean(ret['thresholds'])\n",
        "    pred, p_latency = adjust_predicts(score, label, pot_th, calc_latency=True)\n",
        "    p_t = calc_point2point(pred, label)\n",
        "    print('POT result: ', p_t, pot_th, p_latency)\n",
        "    return {\n",
        "        'pot-f1': p_t[0],\n",
        "        'pot-precision': p_t[1],\n",
        "        'pot-recall': p_t[2],\n",
        "        'pot-TP': p_t[3],\n",
        "        'pot-TN': p_t[4],\n",
        "        'pot-FP': p_t[5],\n",
        "        'pot-FN': p_t[6],\n",
        "        'pot-threshold': pot_th,\n",
        "        'pot-latency': p_latency\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}