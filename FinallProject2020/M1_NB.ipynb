{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of M1-NB",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2r3hM09seYOCfB0tYRrW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangxuanWu/Python/blob/master/FinallProject2020/M1_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yQ0bbVBZ-af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2dffff-e3b2-4849-9b9f-721f0f98bf6e"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEaBHlMe5kGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ef16db-38c9-493e-a079-5514d35bf75f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " all_data.csv\t    importance_list_all_data.csv\n",
            " attacks\t    importance_list_for_attack_files.csv\n",
            "'Colab Notebooks'   results\n",
            " CSVs\t\t   'Yangxuan Wu.zip'\n",
            " feaure_pics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HADKqn_Hmu_o"
      },
      "source": [
        "#import the tool library in whole sections\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKI5ecmaObH-"
      },
      "source": [
        "The following program uses the attack files under the \"./attacks/\" folder as a dataset. The features used are the 4 features with the highest weight for each file, produced by the feature_selection_for_attack_files file. This file applies NB machine learning algorithms to each file 10 times and prints the results of these operations on the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnZXGhie0aoo"
      },
      "source": [
        "#retrieve data\n",
        "csv_files=os.listdir(\"attacks\")# Gets the file name in the attack folder and assigns it to a list (csv_files)ã€‚\n",
        "path=\".\\\\attacks\\\\\"\n",
        "repetition=10"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OkDOBiF0iRY"
      },
      "source": [
        "#Build Naive Bayes Model\n",
        "ml_list={\"Naive Bayes\":GaussianNB()}#The machine learning algorithms to be used are defined in a dictionary (ml_list)."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GPgItm_0rF2"
      },
      "source": [
        "# the features to be used for each attack type is defined in a dictionary(features).\n",
        "# the first 4 of the features created by the file \"04_1_feature_selection_for_attack_files.py\" are used here.\n",
        "features={\"Bot\":[\"Bwd Packet Length Mean\",\"Flow IAT Max\",\"Flow Duration\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DDoS\":[\"Bwd Packet Length Std\",\"Total Backward Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"DoS GoldenEye\":[\"Flow IAT Max\",\"Bwd Packet Length Std\",\"Flow IAT Min\",\"Total Backward Packets\",\"Label\"],\n",
        "\"DoS Hulk\":[\"Bwd Packet Length Std\",\"Fwd Packet Length Std\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Label\"],\n",
        "\"DoS Slowhttptest\":[\"Flow IAT Mean\",\"Fwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Total Length of Bwd Packets\",\"Label\"],\n",
        "\"DoS slowloris\":[\"Flow IAT Mean\",\"Total Length of Bwd Packets\",\"Bwd Packet Length Mean\",\"Total Fwd Packets\",\"Label\"],\n",
        "\"FTP-Patator\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Std\",\"Fwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Label\"],\n",
        "\"Heartbleed\":[\"Total Backward Packets\",\"Fwd Packet Length Max\",\"Flow IAT Min\",\"Bwd Packet Length Max\",\"Label\"],\n",
        "\"Infiltration\":[\"Fwd Packet Length Max\",\"Fwd Packet Length Mean\",\"Flow Duration\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"PortScan\":[\"Flow Bytes/s\",\"Total Length of Fwd Packets\",\"Fwd IAT Total\",\"Flow Duration\",\"Label\"],\n",
        "\"SSH-Patator\":[\"Fwd Packet Length Max\",\"Flow Duration\",\"Flow IAT Max\",\"Total Length of Fwd Packets\",\"Label\"],\n",
        "\"Web Attack\":[\"Bwd Packet Length Std\",\"Total Length of Fwd Packets\",\"Flow Bytes/s\",\"Flow IAT Max\",\"Label\"]}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJKzHSUS0vZi"
      },
      "source": [
        "#time stamp for all processing time\n",
        "seconds=time.time()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bVj5GxD5kJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1424d306-63fa-4c2f-c057-847248c4c65b"
      },
      "source": [
        " #this loop runs on the list containing the filenames. Operations are repeated for all attack files:\n",
        "#for j in csv_files:\n",
        "print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "    \n",
        "    #read each attack file:\n",
        "for j in csv_files: \n",
        "    feature_list=list(features[j[0:-4]])\n",
        "    df=pd.read_csv(path+j, usecols=feature_list)\n",
        "\n",
        "    #Solve the problem of data hollow value and extreme value \n",
        "    df=df.fillna(0)\n",
        "    for col in feature_list:\n",
        "      m = df.loc[df[col] != np.inf, col].max()\n",
        "      df[col].replace(np.inf, m, inplace = True)\n",
        "\n",
        "    #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm:\n",
        "    attack_or_not=[]\n",
        "    for i in df[\"Label\"]: \n",
        "        \n",
        "        if i ==\"BENIGN\":\n",
        "            attack_or_not.append(1)\n",
        "        else:\n",
        "            attack_or_not.append(0)  \n",
        "\n",
        "    df[\"Label\"]=attack_or_not\n",
        "\n",
        "    #this section separates the label and the data into two separate pieces, as Label=y Data=X:\n",
        "    y = df[\"Label\"]  \n",
        "    del df[\"Label\"]\n",
        "    feature_list.remove('Label')\n",
        "    X = df[feature_list]\n",
        "\n",
        "    #this loop runs on the list containing the machine learning algorithm names:\n",
        "    for ii in ml_list: \n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        accuracy=[]\n",
        "        t_time=[]\n",
        "\n",
        "        # This loop allows cross-validation and machine learning algorithm to be repeated 10 times:\n",
        "        for i in range(repetition): \n",
        "            second=time.time()#time stamp for processing time\n",
        "\n",
        "            # cross-validation:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = repetition)\n",
        "            # data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train,%20 test). \n",
        "            # So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "\n",
        "            #machine learing algorithm is applied in this section:\n",
        "            #choose algorithm from ml_list dictionary \n",
        "            clf = ml_list[ii]                                                                        \n",
        "            clf.fit(X_train, y_train)\n",
        "            predict =clf.predict(X_test)\n",
        "        \n",
        "            #makes \"classification report\" and assigns the precision, f-measure, and recall values:   \n",
        "            f_1=f1_score(y_test, predict, average='macro')\n",
        "            pr=precision_score(y_test, predict, average='macro')\n",
        "            rc=recall_score(y_test, predict, average='macro')\n",
        "            precision.append(float(pr))\n",
        "            recall.append(float(rc))\n",
        "            f1.append(float(f_1))\n",
        "            accuracy.append(clf.score(X_test, y_test))\n",
        "            t_time.append(float((time.time()-second)))\n",
        "\n",
        "        #the result of the ten repetitions is printed on the screen:\n",
        "        print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (j[0:-4],ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))\n",
        "        \n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File              ML algorithm       accuracy        Precision       Recall          F1-score        Time           \n",
            "Bot               Naive Bayes        0.56            0.7             0.69            0.56            0.0088         \n",
            "DDoS              Naive Bayes        0.77            0.73            0.68            0.7             0.0775         \n",
            "DoS GoldenEye     Naive Bayes        0.83            0.82            0.75            0.77            0.0229         \n",
            "DoS Hulk          Naive Bayes        0.34            0.65            0.54            0.31            0.4404         \n",
            "DoS Slowhttptest  Naive Bayes        0.42            0.61            0.57            0.41            0.0152         \n",
            "DoS slowloris     Naive Bayes        0.43            0.67            0.59            0.41            0.0147         \n",
            "FTP-Patator       Naive Bayes        1.0             1.0             1.0             1.0             0.0174         \n",
            "Heartbleed        Naive Bayes        1.0             1.0             1.0             1.0             0.0057         \n",
            "Infiltration      Naive Bayes        0.92            0.95            0.86            0.89            0.006          \n",
            "PortScan          Naive Bayes        0.42            0.66            0.59            0.41            0.3045         \n",
            "SSH-Patator       Naive Bayes        0.4             0.67            0.57            0.38            0.0151         \n",
            "Web Attack        Naive Bayes        0.72            0.76            0.8             0.72            0.0086         \n",
            "mission accomplished!\n",
            "Total operation time: =  339.69268107414246 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8hz_GQMVULK"
      },
      "source": [
        "The following program uses the attack files under the \"./attacks/\" folder as a dataset. The features used are the 4 features with the highest weight for each file, produced by the feature_selection_for_attack_files file. This file applies NB machine learning algorithms to each file 10 times and prints the results of these operations on the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU4i2zSZVTFe"
      },
      "source": [
        "#retrieve data:\n",
        "csv_files=[\"all_data.csv\"]# CSV files names: #The names of the dataset files (csv_files).\n",
        "\n",
        "repetition=10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ranryYVgIH"
      },
      "source": [
        "#The machine learning algorithms to be used are defined in a dictionary (ml_list).\n",
        "ml_list={\n",
        "\"Naive Bayes\":GaussianNB()}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs5_4X8QVmnj"
      },
      "source": [
        "#The choosing feature for all_data.csv:\n",
        "features={\"all_data\":[\"Bwd Packet Length Max\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\"Flow Bytes/s\",\n",
        "\"Flow Duration\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow IAT Min\",\"Flow IAT Std\",\"Fwd IAT Total\",\"Fwd Packet Length Max\",\n",
        "\"Fwd Packet Length Mean\",\"Fwd Packet Length Min\",\"Fwd Packet Length Std\",\"Total Backward Packets\",\"Total Fwd Packets\",\n",
        "\"Total Length of Bwd Packets\",\"Total Length of Fwd Packets\",\"Label\"]}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M13oYhIV83e"
      },
      "source": [
        "#time stamp for all processing time:\n",
        "seconds=time.time()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj446M0CaFaX"
      },
      "source": [
        "#Read all_data.cav\n",
        "feature_list=list(features[\"all_data\"])\n",
        "df=pd.read_csv(\"all_data.csv\",usecols=feature_list)#read an attack file."
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBqyGps0QF7D"
      },
      "source": [
        "#Solve the problem of data hollow value and extreme value\n",
        "df=df.fillna(0)\n",
        "for col in feature_list:\n",
        "  m = df.loc[df[col] != np.inf, col].max()\n",
        "  df[col].replace(np.inf, m, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLorQZ-GQJsu"
      },
      "source": [
        "#it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
        "attack_or_not=[]\n",
        "for i in df[\"Label\"]:         \n",
        "  if i ==\"BENIGN\":\n",
        "    attack_or_not.append(1)\n",
        "  else:\n",
        "    attack_or_not.append(0)           \n",
        "df[\"Label\"]=attack_or_not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5caTbyfQMDZ"
      },
      "source": [
        "#this section separates the label and the data into two separate pieces, as Label=y Data=X \n",
        "y = df[\"Label\"]\n",
        "del df[\"Label\"]\n",
        "feature_list.remove('Label')\n",
        "X = df[feature_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0rGznIQPVvD",
        "outputId": "7de714cd-1646-4ca4-b5ae-28fe179a080c"
      },
      "source": [
        "print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"File\",\"ML algorithm\",\"accuracy\",\"Precision\", \"Recall\" , \"F1-score\",\"Time\"))# print output header\n",
        "#this loop runs on the list containing the machine learning algorithm name.\n",
        "for ii in ml_list: \n",
        "  precision=[]\n",
        "  recall=[]\n",
        "  f1=[]\n",
        "  accuracy=[]\n",
        "  t_time=[]\n",
        "\n",
        "  # This loop allows cross-validation and machine learning algorithm to be repeated 10 times:\n",
        "  for i in range(repetition):\n",
        "    second=time.time()#time stamp for processing time\n",
        "\n",
        "    # cross-validation:\n",
        "    # data (X) and labels (y) are divided into 2 parts to be sent to the machine learning algorithm (80% train, %20 test).\n",
        "    # So, in total there are 4 tracks: training data(X_train), training tag (y_train), test data(X_test) and test tag(y_test).\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = repetition)\n",
        "\n",
        "    #machine learning algorithm is applied in this section\n",
        "    #choose algorithm from ml_list dictionary\n",
        "    clf = ml_list[ii]                                                                          \n",
        "    clf.fit(X_train, y_train)\n",
        "    predict =clf.predict(X_test)\n",
        "        \n",
        "    #makes \"classification report\" and assigns the precision, f-measure, and recall values:    \n",
        "    f_1=f1_score(y_test, predict, average='macro')\n",
        "    pr=precision_score(y_test, predict, average='macro')\n",
        "    rc=recall_score(y_test, predict, average='macro')\n",
        "                      \n",
        "    precision.append(float(pr))\n",
        "    recall.append(float(rc))\n",
        "    f1.append(float(f_1))\n",
        "    accuracy.append(clf.score(X_test, y_test))\n",
        "    t_time.append(float((time.time()-second)))\n",
        "\n",
        "  #the result of the ten repetitions is printed on the screen:     \n",
        "  print ('%-17s %-17s  %-15s %-15s %-15s %-15s %-15s' % (\"all_data\",ii,str(round(np.mean(accuracy),2)),str(round(np.mean(precision),2)),str(round(np.mean(recall),2)),str(round(np.mean(f1),2)),str(round(np.mean(t_time),4))))\n",
        "\n",
        "print(\"mission accomplished!\")\n",
        "print(\"Total operation time: = \",time.time() - seconds ,\"seconds\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File              ML algorithm       accuracy        Precision       Recall          F1-score        Time           \n",
            "all_data          Naive Bayes        0.79            0.64            0.64            0.64            2.8683         \n",
            "mission accomplished!\n",
            "Total operation time: =  637.9169425964355 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}